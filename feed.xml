<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://dorianbrown.dev/feed.xml" rel="self" type="application/atom+xml" /><link href="https://dorianbrown.dev/" rel="alternate" type="text/html" /><updated>2021-05-03T13:57:49+02:00</updated><id>https://dorianbrown.dev/feed.xml</id><title type="html">Dorian Brown</title><subtitle>Finding signal and escaping the noise</subtitle><entry><title type="html">What is Supervised Learning? A Mathematical Perspective</title><link href="https://dorianbrown.dev/what-is-supervised-learning/" rel="alternate" type="text/html" title="What is Supervised Learning? A Mathematical Perspective" /><published>2019-09-03T00:00:00+02:00</published><updated>2019-09-03T00:00:00+02:00</updated><id>https://dorianbrown.dev/what-is-supervised-learning</id><content type="html" xml:base="https://dorianbrown.dev/what-is-supervised-learning/">&lt;blockquote&gt;
  &lt;p&gt;This post is the first of a series of posts that serve as an introduction to the field of Machine Learning for those with a mathematical background. They’ve been based off of the Cornell “Machine Learning for Intelligent Systems” course, which has generously put &lt;a href=&quot;https://www.youtube.com/playlist?list=PLl8OlHZGYOQ7bkVbuRthEsaLr7bONzbXS&quot;&gt;all the classes on youtube&lt;/a&gt;. My thanks to Kilian Weinberger for doing a great job teaching this course and for finding a perfect balance of theory and application.&lt;/p&gt;

  &lt;p&gt;We assume some basic knowledge of linear algebra, probability theory, and optimization theory. I’ll try and include links and explanations with the more exotic topics where necessary.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;The goal in supervised learning is to make predictions from data. We start with an initial dataset for which we know what the outcome should be, and our algorithms try and recognize patterns in the data which are unique for each outcome. For example, one popular application of supervised learning is email spam filtering. Here, an email (the data instance) needs to be classified as spam or not-spam.&lt;/p&gt;

&lt;p&gt;Following the approach of traditional computer science, one might be tempted to write a carefully designed program that follows some rules to decide if an email is spam or not. Although such a program might work reasonably well for a while, it has significant drawbacks. As email spam changes the program would have to be rewritten. Spammers could attempt to reverse engineer the software and design messages that circumvent it. And even if it is successful, it could probably not easily be applied to different languages.&lt;/p&gt;

&lt;figure class=&quot;image&quot; style=&quot;text-align: center;&quot;&gt;
  &lt;img style=&quot;max-width: 75%&quot; src=&quot;/assets/images/ml_course/spam_diagram.png&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Machine Learning uses a different approach to generate a program that can make predictions from data. Instead of programming it by hand it is learned from past data. This process works if we have data instances for which we know exactly what the right prediction would have been. For example past data might be user-annotated as spam or not-spam. A machine learning algorithm can utilize such data to learn a program, a classifier, to predict the correct label of each annotated data instance.&lt;/p&gt;

&lt;p&gt;Other successful applications of machine learning include web-search ranking (predict which web-page the user will click on based on his/her search query), placing of online advertisements (predict the expected revenue of an ad, when placed on a homepage, which is seen by a specific user), visual object recognition (predict which object is in an image - e.g. a camera mounted on a self-driving car), face-detection (predict if an image patch contains a human face or not).&lt;/p&gt;

&lt;h2 id=&quot;the-basics&quot;&gt;The Basics&lt;/h2&gt;

&lt;p&gt;All supervised learning algorithms start with some dataset \(D = \{(\textbf{x}_1,y_1),\dots,(\textbf{x}_n,y_n)\}\), where each \(x_i\) is a d-dimensional input or feature vector, and \(y_i\) the corresponding output we call our label. We assume that these data points are drawn from some unknown distribution \(\mathcal{P}\), so&lt;/p&gt;

\[(\textbf{x}_i,y_i) \sim \mathcal{P}\]

&lt;p&gt;where we want want our \((\textbf{x}_i,y_i)\) to be independent and identically distributed (called iid).&lt;/p&gt;

&lt;p&gt;This can be formalized by saying:&lt;/p&gt;

\[D = \{(\textbf{x}_1,y_1),\dots,(\textbf{x}_n,y_n)\} \subseteq \mathbb{R}^d \times \mathcal{C}\]

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(n\) is the size of our dataset&lt;/li&gt;
  &lt;li&gt;\(\mathbb{R}^d\) is the d-dimensional feature space&lt;/li&gt;
  &lt;li&gt;\(\textbf{x}_i\) is the feature vector of the \(i^{th}\) example&lt;/li&gt;
  &lt;li&gt;\(y_i\) is the label or output of the \(i^{th}\) example&lt;/li&gt;
  &lt;li&gt;\(\mathcal{C}\) is the space of all possible labels, or label space.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can sum up the goal of supervised machine learning as finding a function \(h:\mathbb{R}^d \rightarrow \mathcal{C}\), such that for every new input/output pair \((\textbf{x},y)\) sampled from \(\mathcal{P}\) we have \(h(\textbf{x}) \approx y\).&lt;/p&gt;

&lt;p&gt;Let’s see if we understand the definitions above by first looking at a few examples of feature spaces and label spaces.&lt;/p&gt;

&lt;h3 id=&quot;label-spaces-examples&quot;&gt;Label Spaces Examples&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Binary Classification&lt;/em&gt;: Say we’re building a spam filter. Here we have to classes, spam and not spam. The label space is often \(\{0,1\}\) or \(\{-1,1\}\). The choice impacts how we write our loss function, but we’ll see more on that later on.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Multi-class Classification&lt;/em&gt;: If we want to build an image classifier, we need to specify which classes we’re interested in (e.g. 1=&lt;em&gt;horse&lt;/em&gt;, 2=&lt;em&gt;dog&lt;/em&gt;, etc.). If we have \(K\) image classes, we have \(\mathcal{C}=\{1,2,\dots,K\}\).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Regression&lt;/em&gt;: If we want to predict the daily temperature, we’re predicting a number which could take any value, even if some are highly improbable. In this case \(\mathcal{C} = \mathbb{R}\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;feature-spaces-examples&quot;&gt;Feature Spaces Examples&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;House&lt;/em&gt;: If we’re building a model to predict house sale prices, we might take \(\textbf{x}_i = (x_i^1,x_i^2,\dots,x_i^d)\) where \(x_i^1\) is the surface area in \(m^2\), \(x_i^2\) is the number of years ago the house was built, longitude and latitude, etc. In this case we have “hand-crafted” features, each chosen by the modeler.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Text Document&lt;/em&gt;: For something like email classification, a common feature space is the so called bag-of-words. First we find all \(d\) unique words over all the documents we have. We then create the vector \(\textbf{x}_i = (x_i^1,x_i^2,\dots,x_i^d)\) for each document \(i\), where each element \(x_i^j\) tells us how often word \(j\) appears in document \(i\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hypothesis-classes-and-no-free-lunch&quot;&gt;Hypothesis Classes and No Free Lunch&lt;/h2&gt;

&lt;p&gt;There are some steps we need to take on our path to finding that mysterious function $h$. A very important one is that we need to make some assumptions on what the $h$ looks like, and what space of functions we’ll be looking in. This could be linear functions, decision trees, polynomials, or whatever. These are called &lt;em&gt;hypothesis spaces&lt;/em&gt;, usually denoted with $\mathcal{H}$. We need to make this assumption, since this choice has a big impact on how our model will generalize to new data points which aren’t present in our training data, which is the ultimate goal of machine learning.&lt;/p&gt;

&lt;p&gt;One way of understanding these hypothesis spaces is to look at how expressive they are, or how flexible they are in capturing the structure of a dataset. One of the challenges of machine learning is finding the right balance of flexibility, where not enough causes &lt;em&gt;underfitting&lt;/em&gt; or &lt;em&gt;bias&lt;/em&gt;, and too much causes &lt;em&gt;overfitting&lt;/em&gt; or &lt;em&gt;variance&lt;/em&gt;. One way of describing this expressiveness is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension&quot;&gt;Vapnik–Chervonenkis dimension&lt;/a&gt;.&lt;/p&gt;

&lt;figure class=&quot;image&quot; style=&quot;text-align: center;&quot;&gt;
  &lt;img style=&quot;&quot; src=&quot;/assets/images/ml_course/hypothesis_space_impact.png&quot; alt=&quot;Here we see the impact of choosing our hypothesis space, also known as the bias-variance tradeoff. If our space is too large (picture on right) we get fantastic results on our training set, but horrible generalization. If our space is too small (picture on left), we get bad results on the training set, and bad generalization.&quot; /&gt;
  &lt;figcaption&gt;Here we see the impact of choosing our hypothesis space, also known as the bias-variance tradeoff. If our space is too large (picture on right) we get fantastic results on our training set, but horrible generalization. If our space is too small (picture on left), we get bad results on the training set, and bad generalization.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Each machine learning algorithm makes assumptions which restrict its search to a specific space of functions. There’s no way around this due to the &lt;a href=&quot;https://www.wikiwand.com/en/No_free_lunch_in_search_and_optimization&quot;&gt;No Free Lunch Theorem&lt;/a&gt;, which you could summarize as “there’s no ultimate ML algorithm which is the best on all problems”.&lt;/p&gt;

&lt;h2 id=&quot;loss-functions&quot;&gt;Loss Functions&lt;/h2&gt;

&lt;p&gt;After having been given a dataset \(D\) and having chosen a hypothesis space \(\mathcal{H}\) of functions we can possibly learn, the next step is to find the best \(h\) in that set. The problem is, how do we define best \(h\)?&lt;/p&gt;

&lt;p&gt;This is where a loss function \(L:\mathcal{H} \rightarrow \mathbb{R}\) comes into play, which assigns a loss to each \(h \in \mathcal{H}\). This loss number tells us how good our \(h\) is, given the data \(D\) we want it to reproduce. In general a loss function will give hypotheses \(h\) with fewer reproduction errors on \(D\) a lower loss, but there are times where you want to take additional factors into account like model complexity. We’ll return to this when we cover regularization.&lt;/p&gt;

&lt;p&gt;With this loss function at hand, our original problem has now become an optimization problem, which is&lt;/p&gt;

\[\arg\min_{h \in \mathcal{H}} L(h) = \arg\min_{h \in \mathcal{H}} \frac{1}{n}\sum_{i=1}^nl(\textbf{x}_i,y|h)\]

&lt;p&gt;We’ll be using \(L\) for the loss of a hypothesis \(h\) given \(D\), and \(l\) for the loss of a single data pair \((\textbf{x}_i,y)\) given \(h\).&lt;/p&gt;

&lt;h3 id=&quot;zero-one-loss&quot;&gt;Zero-One Loss&lt;/h3&gt;

&lt;p&gt;This is one of the simplest loss functions. What this loss does is count the number of mistakes \(h\) makes for each training sample. We can state this as:&lt;/p&gt;

\[L(h) := \frac{1}{n}\sum_{i=1}^n\delta_{h(\textbf{x}_i)\neq y_i}\]

&lt;p&gt;where \(\delta\) is the Dirac delta function&lt;/p&gt;

\[\delta_{h(\textbf{x}_i)\neq y_i} = \begin{cases} 
      1 &amp;amp; h(\textbf{x}_i)\neq y_i \\
      0 &amp;amp; \text{otherwise} \\
   \end{cases}\]

&lt;p&gt;This isn’t used much in practice since it’s non-differentiable and non-continuous, which makes it difficult to work with for optimization algorithms we be needing.&lt;/p&gt;

&lt;h3 id=&quot;squared-loss&quot;&gt;Squared Loss&lt;/h3&gt;

&lt;p&gt;This loss is generally used in regression problems where \(y_i \in \mathbb{R}\). The loss per training sample is \((h(\mathbf{x}_i) - y_i)^2\), which is the distance squared. The overall loss becomes&lt;/p&gt;

\[L(w):=\frac{1}{n}\sum_{i=1}^n(h(\mathbf{x}_i) - y_i)^2\]

&lt;p&gt;The fact that the error is squared means that large errors will be much more punishing than smaller ones, and when searching for \(h \in \mathcal{H}\) we’ll end up choosing one which would rather have lots of small errors rather than a few large ones.&lt;/p&gt;

&lt;h2 id=&quot;generalization&quot;&gt;Generalization&lt;/h2&gt;

&lt;p&gt;One question you might want to ask is “if we’re only looking at our data \(D\), how do we ensure our model will perform well on new data?” We’ll show why this is an important question with a little motivating example.&lt;/p&gt;

&lt;p&gt;Given a data set \(D\), lets define a function \(h\) which just memorizes the output \(y_i\) for each input vector \(\textbf{x}_i\), or:&lt;/p&gt;

\[h(\textbf{x}) := \begin{cases} y_i &amp;amp; \text{if there exists $(\textbf{x}_i,y_i)$ such that $\textbf{x} = \textbf{x}_i$}, \\ 0 &amp;amp; \text{otherwise}\end{cases}\]

&lt;p&gt;This function would perform perfectly on our training data \(D\), but anything new which we haven’t seen before would probably be horribly wrong. So how do we ensure we don’t learn hypotheses like this?&lt;/p&gt;

&lt;h3 id=&quot;train-test-split&quot;&gt;Train Test Split&lt;/h3&gt;

&lt;p&gt;The generally used approach to avoid the above pitfall is to split our dataset \(D\) into three sets, \(D_{TR}, D_{VA}, D_{TE}\), which are usually called train, validation and test. A good split might be something like 80/10/10, although this depends on the application and size of \(D\) among other things.&lt;/p&gt;

&lt;figure class=&quot;image&quot; style=&quot;text-align: center;&quot;&gt;
  &lt;img style=&quot;&quot; src=&quot;/assets/images/ml_course/train_validation_test.png&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;We do this at the very start of our search for \(h\). When we’re trying out all kinds of different algorithms we use the training set to search for our function \(h\), and our validation set to determine if it’s any good. Once we’ve done this numerous times and are happy with the results, we crack open the test set to see what the final accuracy is.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Train/Test Split Gone Wrong&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;We need to be really careful when we’re creating the split in our data. Lets take spam-filtering as an example. If we just randomly split our emails and labels into train/validation/test, create a classifier, we’d we surprised to find that our model has 0% error! Although some might think “Great, we’re done!”, it would be wiser to look into what might be happening.&lt;/p&gt;

  &lt;p&gt;After some detective work, you would discover we made a big error. Spammers create an email once and send it to millions of people. This means we have same email text in both our training set and test set. What our classifier did was just memorize which words are in those spam emails, and since the same text was in the test set it worked great.&lt;/p&gt;

  &lt;p&gt;In reality spammers change their emails frequently to avoid such classifiers, so we should be smarter too. We want our test set to mimick the settings in which our algorithm will have to function in the real world. A simple way to fix this is to split our dataset by time, so we’ll be testing on newer emails than were found in our training set.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;formalizing-generalization&quot;&gt;Formalizing Generalization&lt;/h3&gt;

&lt;p&gt;We can formalize what we mean by generalization with this expression&lt;/p&gt;

\[\mathbb{E}_{(\textbf{x},y)\sim \mathcal{P}}\left[l(h;(\textbf{x},y))\right]\]

&lt;p&gt;which is the expected loss of our hypothesis, if we take the expectation over all possible input/output pairs drawn from \(\mathcal{P}\). This is really what we want to minimize, but we can’t do this directly since we don’t know what \(\mathcal{P}\) is.&lt;/p&gt;

&lt;p&gt;This is what we’re trying to simulate with our validation set \(D_{VA}\). Since our algorithm has never seen this set, it’s as though we’re drawing that many new points from our distribution \(\mathcal{P}\)!&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;We defined our dataset $D$ as a set \(\{(\textbf{x}_1,y_1),\dots,(\textbf{x}_n,y_n)\}\) of data points. Given this dataset, we need to choose a hypothesis space \(\mathcal{H}\) which we will search through to find a good function \(h \in \mathcal{H}\) for which \(h(\textbf{x}) \approx y\) when \((\textbf{x},y) \sim \mathcal{P}\). We generally choose \(\mathcal{H}\) implicitly by our choice of machine learning algorithm.&lt;/p&gt;

&lt;p&gt;Which function \(h\) is best, is decided by our choice of loss function $L$. This allows us to compare the performance of datasets on some set of data points.&lt;/p&gt;

&lt;p&gt;In order to make sure our algorithm generalizes well after training, we split our dataset into trainging, validation and test. We training on the training set, optimize our algorithm according to the loss on the validation set, and once we’re done we use the test set to get a good approximation of how our algorithm will perform in the real world.&lt;/p&gt;</content><author><name></name></author><summary type="html">This post is the first of a series of posts that serve as an introduction to the field of Machine Learning for those with a mathematical background. We'll start here by introducing features, labels, hypothesis spaces, loss functions and model generalization.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://dorianbrown.dev/assets/images/ml_course/supervised_header.jpg" /><media:content medium="image" url="https://dorianbrown.dev/assets/images/ml_course/supervised_header.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Better Command Line Debugging with Python</title><link href="https://dorianbrown.dev/debugging-python/" rel="alternate" type="text/html" title="Better Command Line Debugging with Python" /><published>2019-01-07T00:00:00+01:00</published><updated>2019-01-07T00:00:00+01:00</updated><id>https://dorianbrown.dev/debugging-python</id><content type="html" xml:base="https://dorianbrown.dev/debugging-python/">&lt;h2 id=&quot;debugging-the-hard-way&quot;&gt;Debugging the Hard Way&lt;/h2&gt;

&lt;p&gt;For those of you who are terminal fanatics, and love running their code from the command line, debugging can sometimes be a pain. Often my debugging workflow would have been (when not working in PyCharm), reading the error, finding the file and line number, opening the respective script and adding a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pdb.set_trace&lt;/code&gt; statement, and then rerunning the script.&lt;/p&gt;

&lt;p&gt;This long process doesn’t just take up some precious time, but it interrupts your mental flow. So maybe there’s a better way…&lt;/p&gt;

&lt;h2 id=&quot;post-mortem-debugging&quot;&gt;Post Mortem Debugging&lt;/h2&gt;

&lt;p&gt;This &lt;a href=&quot;https://stackoverflow.com/questions/242485/starting-python-debugger-automatically-on-error&quot;&gt;stackoverflow discussion&lt;/a&gt; gives a great solution to our problem. All you have to do is run your problematic script in the following way:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;monokai&quot;&gt;&lt;code&gt;python &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; pdb &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;continue &lt;/span&gt;error.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will run the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;error.py&lt;/code&gt; script, and if any errors are encountered it stops and launches an interactive python shell at the moment where the error happens.&lt;/p&gt;

&lt;p&gt;Here’s an example of the output:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;monokai&quot;&gt;&lt;code&gt;Traceback &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;most recent call last&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;:
  File &lt;span class=&quot;s2&quot;&gt;&quot;/home/dorian/miniconda3/lib/python3.7/pdb.py&quot;&lt;/span&gt;, line 1697, &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;main
    pdb._runscript&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;mainpyfile&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
  File &lt;span class=&quot;s2&quot;&gt;&quot;/home/dorian/miniconda3/lib/python3.7/pdb.py&quot;&lt;/span&gt;, line 1566, &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;_runscript
    self.run&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;statement&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
  File &lt;span class=&quot;s2&quot;&gt;&quot;/home/dorian/miniconda3/lib/python3.7/bdb.py&quot;&lt;/span&gt;, line 585, &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;run
    &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;cmd, globals, locals&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
  File &lt;span class=&quot;s2&quot;&gt;&quot;&amp;lt;string&amp;gt;&quot;&lt;/span&gt;, line 1, &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &amp;lt;module&amp;gt;
  File &lt;span class=&quot;s2&quot;&gt;&quot;/home/dorian/error.py&quot;&lt;/span&gt;, line 3, &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &amp;lt;module&amp;gt;
    def throw_error&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;:
  File &lt;span class=&quot;s2&quot;&gt;&quot;/home/dorian/error.py&quot;&lt;/span&gt;, line 5, &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;throw_error
    1 + my_list
TypeError: unsupported operand &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;s&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; +: &lt;span class=&quot;s1&quot;&gt;'int'&lt;/span&gt; and &lt;span class=&quot;s1&quot;&gt;'list'&lt;/span&gt;
Uncaught exception. Entering post mortem debugging
Running &lt;span class=&quot;s1&quot;&gt;'cont'&lt;/span&gt; or &lt;span class=&quot;s1&quot;&gt;'step'&lt;/span&gt; will restart the program
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; /home/dorian/error.py&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;5&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;throw_error&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
-&amp;gt; 1 + my_list
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Pdb&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is great! Although we still have to rerun the script, we no longer need to dive into the code and set our breakpoints with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pdb.set_trace&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;less-typing-with-bash-aliases&quot;&gt;Less typing with bash aliases&lt;/h2&gt;

&lt;p&gt;Although using this saves some steps, we still need to remember this pretty verbose command. If you aren’t already familiar with bash aliases, this is as good a time as any to start using them.&lt;/p&gt;

&lt;p&gt;All you need to do is add the following line to either your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.bashrc&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.bash_aliases&lt;/code&gt; file:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;monokai&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;alias &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;pydebug&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;python -m pdb -c continue&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now instead of typing that whole string of commands, we can just call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pydebug error.py&lt;/code&gt; and we get the same debugging magic.&lt;/p&gt;

&lt;p&gt;Remember that you need to reload your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.bashrc&lt;/code&gt; file before your aliases work. Just run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;source .bashrc&lt;/code&gt; and you’re ready to go&lt;/p&gt;

&lt;h2 id=&quot;adding-some-functionality-to-pdb&quot;&gt;Adding some functionality to pdb&lt;/h2&gt;

&lt;p&gt;Most of you probably don’t work in the basic python shell too much, and I personally am pretty hopeless without intelligent code completion. If you want all these wonderful things in this debugging shell, you’re in luck! You just need to replace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pdb&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ipdb&lt;/code&gt; and you get an IPython shell instead of the basic python one. Although &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pdb&lt;/code&gt; is included with python itself, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ipdb&lt;/code&gt; needs to be pip installed.&lt;/p&gt;</content><author><name></name></author><summary type="html">How to automatically create an interactive python shell when errors occur in your script.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://dorianbrown.dev/assets/images/pydebug/debug.png" /><media:content medium="image" url="https://dorianbrown.dev/assets/images/pydebug/debug.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">A Little Math on Logistic Regression</title><link href="https://dorianbrown.dev/logistic-regression/" rel="alternate" type="text/html" title="A Little Math on Logistic Regression" /><published>2018-10-29T00:00:00+01:00</published><updated>2018-10-29T00:00:00+01:00</updated><id>https://dorianbrown.dev/logistic-regression</id><content type="html" xml:base="https://dorianbrown.dev/logistic-regression/">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;One of the most popular techniques for classification is the binary classification method. It’s simplicity, interpretability, and scalability are big advantages. Although more recent methods for classification have gotten a fair amount of attention the logistic regression stays a strong contender for many problems.&lt;/p&gt;

&lt;p&gt;I wrote this article because of a training I gave on the subject for some junior data scientists. Although I had worked with it before, I never took the time to understand some of it’s “finer” points. Coming from a mathematics background, I like to understand how things work and why they work. I feel this helps in understanding what the strengths and weaknesses of something are, and how it’s connected to similar mathematical machinery. For those of you who are interested, here are some of the things I discovered.&lt;/p&gt;

&lt;p&gt;To keep things simple we’ll be limiting ourselves to the two (e.g. 0 and 1) class problem.&lt;/p&gt;

&lt;h1 id=&quot;relation-between-thresholds-and-decision-boundaries&quot;&gt;Relation between thresholds and decision boundaries&lt;/h1&gt;

&lt;p&gt;Most of you probably know that in order to get the binary output we want, we must choose a threshold value. Probabilities below this threshold are set to 0, and above it are set to 1. Visually we can think of it this way:&lt;/p&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;/assets/images/logreg/prob_threshold.png&quot; style=&quot;width: 85%;&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;You might wonder, how does that relate to a decision boundary? The decision boundary refers to the hypersurface that splits the feature space into the two output classes. Which side of the boundary a point is determines how it gets classified by the model.&lt;/p&gt;

&lt;p&gt;Given a probability threshold, it’s pretty easy to find this boundary with a little linear algebra. Let’s take $t = 1/2$ and see what what the boundary is:&lt;/p&gt;

\[\begin{align} P(Y = 1 | X = x) &amp;amp; = \frac{1}{1 + e^{\beta^\intercal x}} = 1/2 \end{align}\]

&lt;p&gt;We can rewrite this as&lt;/p&gt;

\[\begin{align}\frac{1}{1 + e^{\beta^\intercal x}} &amp;amp; = \frac{1}{2}
    \\ 1 + e^{\beta^\intercal x} &amp;amp; = 2 
    \\ e^{\beta^\intercal x} &amp;amp; = 1
    \\ \beta^\intercal x &amp;amp; = 0\end{align}\]

&lt;p&gt;So there it is, our boundary for $t=0.5$ is the line $\{x \in \mathbb{R}^n: \beta^{\intercal}x = 0 \}$, which is a hyperplane in $\mathbb{R}^n$. If we include nonlinear functions of our features as additional features, this will add nonlinearities to this decision boundary.&lt;/p&gt;

&lt;h1 id=&quot;deriving-cross-entropy-from-maximum-likelihood&quot;&gt;Deriving Cross-Entropy from Maximum Likelihood&lt;/h1&gt;

&lt;p&gt;So now that we’ve defined the model, we also need a function which tells us how well the model fits the data we’re training on. There are lots of loss functions to choose from, so why is the &lt;a href=&quot;https://www.wikiwand.com/en/Cross_entropy&quot;&gt;cross-entropy&lt;/a&gt; usually taken for logistic regression?&lt;/p&gt;

&lt;p&gt;The short answer is it’s equivalent to the &lt;a href=&quot;https://www.wikiwand.com/en/Maximum_likelihood_estimation&quot;&gt;Maximum Likelihood Estimator&lt;/a&gt; here, which has all kinds of nice properties. It’s pretty easy to derive the cross-entropy from the MLE, so we’ll show that here. Lets start with the expression of our trustworthy log-likelihood:&lt;/p&gt;

\[\begin{align} \log \mathcal{L}(\beta) &amp;amp; = \sum_{i=1}^n\log P_\beta(Y = y_i|X=x_i)
    \\ &amp;amp; = \sum_{i=1}^n\log P_\beta(Y = 1|X=x_i)^{y_i} + \log P_\beta(Y = 0|X=x_i)^{(1 - y_i)} 
    \\ &amp;amp; = \sum_{i=1}^n y_i\log \hat{y}_\beta(x_i) + (1 - y_i)\log (1 - \hat{y}_\beta(x_i))  \end{align}\]

&lt;p&gt;which is exactly the cross-entropy loss.&lt;/p&gt;

&lt;h1 id=&quot;geometric-interpretation-of-l1l2-regularization&quot;&gt;Geometric Interpretation of L1/L2 Regularization&lt;/h1&gt;

&lt;p&gt;L1/L2 regularization (also known as Ridge/Lasso) is a widely used technique for reducing model overfitting. We restrict the size of the model’s weights, which restricts how complex the model can become. This increases bias, reduces variance, and allows for better generalization on the test set.&lt;/p&gt;

&lt;p&gt;In order to regularize our problem, we minimize the original problem:&lt;/p&gt;

\[\sum_{i=1}^n L(\beta, x_i) = \sum_{i=1}^n y_i\log \hat{y}_\beta(x) + (1 - y_i)\log (1 - \hat{y}_\beta(x_i))\]

&lt;p&gt;but we add a restriction, namely:&lt;/p&gt;

\[\text{arg}\,\text{min}_\beta\sum_{i=1}^n L(\beta, x_i), \quad \text{such that}\, \|\beta\| \lt C\]

&lt;p&gt;for some $C &amp;gt; 0$ which determines how strong we want to regularize. If we use the L2-norm for $\Vert\beta\Vert$ we get Ridge regularization, and with the L1-norm we get Lasso. Smaller values for $C$ correspond with stronger regularization.&lt;/p&gt;

&lt;h2 id=&quot;feature-selection-with-l1l2-regularization&quot;&gt;Feature Selection with L1/L2 regularization&lt;/h2&gt;

&lt;p&gt;A key difference between the two kinds of regularization is that lasso “turns-off” features, while ridge only reduces their values. Why this happens has a nice geometric explanation which I took from &lt;a href=&quot;https://web.stanford.edu/~hastie/ElemStatLearn/&quot;&gt;Elements of Statistical Learning&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;ridge&quot;&gt;Ridge&lt;/h3&gt;

&lt;p&gt;It can be shown for logistic regression with cross-entropy loss that the loss function is convex, with elliptical level sets. The gives the optimization problem shown below when $\beta = (w_1, w_2)$:&lt;/p&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;/assets/images/logreg/l2_regularization.png&quot; style=&quot;width: 75%&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;In this picture, you can see that if we’re using the L2-norm, the unit-ball takes the form of a circle. This means that the minimum will be where the black dot is, and this will usually be at a point where $w_1, w_2 &amp;gt; 0$, and as we wanted both fitted weights are smaller than the original one.&lt;/p&gt;

&lt;p&gt;To see this happening with a real model, take a look at this graph which plots the value of various weights as we increase the amount of regularization by reducing $\text{df}(\lambda)$.&lt;/p&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;/assets/images/logreg/ridge_coefficients.png&quot; width=&quot;75%&quot; /&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;lasso&quot;&gt;Lasso&lt;/h3&gt;

&lt;p&gt;Things work a little differently for Lasso regularization with the L1-norm. In this case the unit ball looks like a diamond, which causes the constrained minimum to more often fall on the corners of the diamond. These corners correspond with features being set to zero.&lt;/p&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;/assets/images/logreg/l1_regularization.png&quot; width=&quot;75%&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;It’s a bit harder to visualize this picture in higher dimensions, but just like the ridge regression we can plot the weight values against the regularization strength to visualize this “turning-off” of features.&lt;/p&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;/assets/images/logreg/lasso_coefficients.png&quot; width=&quot;75%&quot; /&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;lagrange-duals-and-regularization&quot;&gt;Lagrange Duals and Regularization&lt;/h2&gt;

&lt;p&gt;The last thing I want to share is a detail on regularization I saw lot’s of authors skip over. Usually regularization is written as this minimization problem, the one with regularization term added to the loss function&lt;/p&gt;

\[\text{arg}\,\text{min}_\beta\left(\sum_{i=1}^n L(\beta, x_i) + \lambda \Vert\beta\Vert\right)\]

&lt;p&gt;I wanted to figure out why this was equivalent to&lt;/p&gt;

\[\text{arg}\,\text{min}_\beta\sum_{i=1}^n L(\beta, x_i), \quad \text{such that}\, \|\beta\| \lt C.\]

&lt;p&gt;The second is essential to the whole geometric interpretation, while the first is the form used for gradient descent, so I wanted to see this equivalence for myself.&lt;/p&gt;

&lt;p&gt;After some research, I found out these two formulations are equivalent due the &lt;a href=&quot;https://www.wikiwand.com/en/Duality_(optimization)&quot;&gt;strong Lagrangian principle&lt;/a&gt;. For the curious, this comes from the field of convex optimization and tells us which conditions are necessary for these two types of problems to be equivalent. For more information on this duality you can check out &lt;a href=&quot;https://pdfs.semanticscholar.org/7aa3/9f7f3b69473705e247dd2b3a9689f10fbbc3.pdf&quot;&gt;this paper&lt;/a&gt; which formally proves this result.&lt;/p&gt;</content><author><name></name></author><summary type="html">Logistic Regression is one of the first techniques taught in Machine Learning, and for many applications is a good baseline model. Here I'd like to share some details I've discovered about it over the last year, which helped me better understand how and why it works.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://dorianbrown.dev/assets/images/logreg/binary-dataset.png" /><media:content medium="image" url="https://dorianbrown.dev/assets/images/logreg/binary-dataset.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Understanding Tensorflow with MNIST</title><link href="https://dorianbrown.dev/mnist-tensorflow/" rel="alternate" type="text/html" title="Understanding Tensorflow with MNIST" /><published>2018-10-07T00:00:00+02:00</published><updated>2018-10-07T00:00:00+02:00</updated><id>https://dorianbrown.dev/mnist-tensorflow</id><content type="html" xml:base="https://dorianbrown.dev/mnist-tensorflow/">&lt;h1 id=&quot;opening-remarks&quot;&gt;Opening Remarks&lt;/h1&gt;

&lt;p&gt;Recently at work we had a hackathon on tensorflow, which is something I’d tried (running examples) but never really took the time to play with it. I’d like to share my experiences with the tensorflow computational graph, and some of the flexibility it has to do cool stuff.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I’m not a tensorflow expert, so please let me know if there are any inaccuracies I need to correct :grin:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;quick-introduction-to-tensorflow&quot;&gt;Quick Introduction to Tensorflow&lt;/h1&gt;

&lt;h2 id=&quot;the-computation-graph&quot;&gt;The computation graph&lt;/h2&gt;

&lt;p&gt;In order to do anything in tensorflow, you need to create a so called computation graph. This is the main construct used to break things down into problems your GPU can efficiently solve. Like Numpy and PySpark, python is just a wrapper for telling other engines to run your computations. In the case of tensorflow, this is for CUDA and your GPU.&lt;/p&gt;

&lt;p&gt;Sending data from your GPU to python and back creates a lot of performance overhead, so the computation graph is a way of helping you do that as little as possible.&lt;/p&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;/assets/images/mnist/comp_graph.gif&quot; style=&quot;width: 75%;&quot; /&gt;
    &lt;figcaption&gt; Example of a tensorflow computation graph&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;You define a computation graph using special tensor objects (eg. constants, vectors, matrices) and tensor operations (eg. multiply, weighted sum, apply loss function). Inputs to your graph (like a training dataset) are defined as placeholders.&lt;/p&gt;

&lt;h2 id=&quot;calculating-a-matrix-inverse-the-hard-way&quot;&gt;Calculating a matrix inverse, the hard way&lt;/h2&gt;

&lt;p&gt;Say we want to make a simple computation graph, but a little more complicated than adding two numbers. Let’s take a square matrix as input, and multiple it by some set matrix $A$. Our goal is to create a graph that finds the matrix inverse $A^{-1}$, which we enforce with the loss. This function tries to make the output as close to the identity matrix as possible.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;monokai&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# We'll put this into the graph later
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand_matrix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# We start making our graph here
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;A&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;id_mat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;identity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;id_mat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Here we define some &quot;meta-graph&quot; stuff
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimiser&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AdagradOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;init_op&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_variables_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So in the above section we defined:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;some variables (stuff which get updated and persists between runs)&lt;/li&gt;
  &lt;li&gt;some constants which don’t change&lt;/li&gt;
  &lt;li&gt;matrix multiplication, which transforms our input into an output in our case&lt;/li&gt;
  &lt;li&gt;a loss function&lt;/li&gt;
  &lt;li&gt;an optimiser which changes the variables in the graph to minimize the loss&lt;/li&gt;
  &lt;li&gt;a variable initializer, which creates initial states for any variables which are defined using a distribution (we don’t use those)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With this defined, we can start training the model.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;monokai&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Number of passes over the data
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100_000&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Context managers work nicely with tensorflow sessions
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_op&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimiser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Lets evaluate the learned matrix A
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;x_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Running the optimizer once will update the variable x by one step. In order to get a good estimate, we run the optimizer 100,000 times. With the last bit of code, we extract the learned matrix x from the tf session and check if it is indeed close to the inverse.&lt;/p&gt;

&lt;p&gt;In order to see if the algorithm is converging we can look at the loss of the model vs the training epoch. Just to be sure we also check to see if the learned matrix x actually does approximate $A^{-1}$ (it does).&lt;/p&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;/assets/images/mnist/example_learning_rate.png&quot; style=&quot;width: 75%;&quot; /&gt;
    &lt;figcaption&gt;Yay, our loss is going down!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;We trained our first tensorflow model! This little excercise helped me understand the concepts of the computation graph, and it should make the later example easier to follow if you’re new to tensorflow.&lt;/p&gt;

&lt;h1 id=&quot;classifying-mnist-with-tensorflow&quot;&gt;Classifying MNIST with tensorflow&lt;/h1&gt;

&lt;p&gt;Now that we understand the basics of tensorflow, lets build single-layered neural network using tensorflow to classify these images.&lt;/p&gt;

&lt;h2 id=&quot;model-definition&quot;&gt;Model Definition&lt;/h2&gt;

&lt;p&gt;We’ll rescale the images to 28x28 pixel images and represent these by a 28x28 matrix, where each matrix entry represents the pixels color-intensity with a number between 0 and 1.&lt;/p&gt;

&lt;p&gt;The output of the network will be a one-hot encoding of the numbers 0-9, so each one will be a 10-dimensional vector.&lt;/p&gt;

&lt;p&gt;Finally, the network connecting the inputs to the outputs will be a single densely connected layer, a relu activation function, and a softmax to ensure we are left with a probability vector as the output.&lt;/p&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;/assets/images/mnist/network_diagram.svg&quot; style=&quot;width: 75%;&quot; /&gt;
    &lt;figcaption&gt;The network architecture of our MNIST classifier&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;Just like the previous example, we’re going to define our network in a computation graph, using tensors and tensor operations. We’ll use a placeholder x for our input images and y for the one-hot encoded label. Note that we reshaped the images from a 28x28 matrix to a vector of length 784.&lt;/p&gt;

&lt;p&gt;Here’s the graph we used for this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;monokai&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow.examples.tutorials.mnist&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_data&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Read data
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mnist&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_data_sets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;MNIST_data/&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;one_hot&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Python optimisation variables
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# declare the training data placeholders
# input x - for 28 x 28 pixels = 784
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;784&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# now declare the output data placeholder - 10 digits
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# now declare the weights connecting the input to the hidden layer
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;784&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stddev&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.03&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stddev&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.03&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# calculate the output of the hidden layer
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hidden_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_clipped&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clip_by_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9999999&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cross_entropy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_clipped&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                         &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_clipped&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# add an optimiser
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimiser&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cross_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# finally setup the initialisation operator
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_op&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_variables_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# define an accuracy assessment operation
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;correct_prediction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;correct_prediction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Training the network
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_op&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;total_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mnist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;avg_cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;batch_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mnist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimiser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cross_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                                                   &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;avg_cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_batch&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Epoch:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;cost =&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;{:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avg_cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mnist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mnist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let’s check out the output of this model, and see if it did what we wanted to. We’ll look at the cross-entropy loss on the training set, test set, and the overall accuracy on the test set, for each epoch.&lt;/p&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;/assets/images/mnist/mnist_output.png&quot; style=&quot;width: 75%;&quot; /&gt;
    &lt;figcaption&gt;How our network learns over time &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This will learn the network weights $W_1, b_1, W_2, b_2$ and give a pretty decent accuracy for classification, 98% on the test set!&lt;/p&gt;

&lt;h1 id=&quot;looking-into-our-network&quot;&gt;Looking into our network&lt;/h1&gt;

&lt;h2 id=&quot;image-fingerprints&quot;&gt;Image fingerprints&lt;/h2&gt;

&lt;p&gt;As this was my first time working with tensorflow and a neural network, I was interested in dissecting the network a bit. The input and output layers aren’t too interesting, so I thought I’d look at the hidden layer for out-of-sample images.&lt;/p&gt;

&lt;p&gt;Getting each hidden layer was quite simple. If you want the hidden layers for all 0 images:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Create a tf-session and train the neural network. Don’t close the session!&lt;/li&gt;
  &lt;li&gt;Filter the test images to only the 0 ones.&lt;/li&gt;
  &lt;li&gt;Running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sess.run(hidden_out, feed_dict={x: input_data})&lt;/code&gt; will return a tensor where each row is the hidden layer for the images that you fed it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you plot this matrix for each label, we get the following image:&lt;/p&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;/assets/images/mnist/hidden_layers.png&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;figcaption&gt;Showing trained neural network hidden layer vectors for each test image&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;So what do we see here? One thing that is pretty clear, is that each class shows clear horizontal bands across all the test images. Each vertical stripe is the hidden layer vector for that specific test set image. If we see a horizontal stripe, that means that vector entry is “activated” in most of the images with that label.&lt;/p&gt;

&lt;p&gt;This unique pattern of bands is what the second bit of linear algebra ($A_2\cdot x + b_2$) uses to classify these images, as the combination is a kind of unique indentifier or fingerprint for each image. Our network learned this itself, which is super cool!&lt;/p&gt;

&lt;h2 id=&quot;estimating-optimal-input-images&quot;&gt;Estimating optimal input images&lt;/h2&gt;

&lt;p&gt;After seeing the patterns in the hidden layers, I was curious what the “otpimal” image would be for each class. The first approach I tried wasn’t very feasible, reversing the network direction to give an input for a specific output. This has to do with the fact that we reduce a $784 \times 1$ vector to a $10 \times 1$ vector, so a lot of information is lost.&lt;/p&gt;

&lt;p&gt;There’s another approach we can use, where we train another model which tries to learn the input vector which best outputs one of the outputs corresponding with the one-hot encoded labels. So we try to find the best image input for every output number, according to the network, or technically we’re finding the image vector $\hat{V}_i$ which minimizes the cross-entropy loss between it’s output $\hat{y}_i$ and the unit vector $e_i$ for every $i =0,1,\cdots,9$. The code for this can be found &lt;a href=&quot;https://github.com/dorianbrown/notebooks/blob/master/mnist_nn.ipynb&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here are the first images I got with this approach. The images on the top are the average pixel density of all the test sampes, and on the bottom are the “ideal images” according to the neural network.&lt;/p&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;/assets/images/mnist/nonideal_outputs.png&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;figcaption&gt;This looks like it might be overfitting...&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;regularization-to-the-rescue&quot;&gt;Regularization to the rescue&lt;/h1&gt;

&lt;p&gt;So it seems that since our input image has 784 free parameters and we’re trying to create a vector of length 10. Our model is seriously overfitting.&lt;/p&gt;

&lt;p&gt;I tried regularization (I wasn’t sure what else to do) and started out with the L1 version. That killed most of the pixels except for a few. L2, on the other hand, was a little more gentle and worked like a charm! Adding a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;penalty*tf.reduce_sum(x**2)&lt;/code&gt; to the cross-entropy term allows us to add a little bias and reduce the variance, giving this much improved version.&lt;/p&gt;

&lt;p&gt;L2 regularization with a penalty of 0.25 gave these optimal images&lt;/p&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;/assets/images/mnist/ideal_outputs.png&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;figcaption&gt;Yay, signal in the noise!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;So we finally get some patterns in our ideal images. In each of the images we can roughly make out each of the numbers, but there are some strange irregularities in them.&lt;/p&gt;

&lt;p&gt;These irregularities might be the pixels the network finds important for distinguishing numbers, but maybe they’re artifacts from the regularization. Pretty neat though, as there are clearly patterns in the unique areas of each number.&lt;/p&gt;

&lt;h1 id=&quot;the-end&quot;&gt;The end&lt;/h1&gt;

&lt;p&gt;For those of you who held out until the end, here’s a :doughnut:. I hope you either read something interesting or learned something new along the way.&lt;/p&gt;

&lt;p&gt;I’m still new to the whole ML-blogging, so if you’ve got any reactions or tips please leave a message in the comments. You would really help me make the blog better!&lt;/p&gt;</content><author><name></name></author><summary type="html">During a hackathon at work I finally had tensorflow and it's api &quot;click&quot; for me. This article shares some of what made it click for me, in addition to the other stuff I did trying to understand the weights of the trained network better.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://dorianbrown.dev/assets/images/mnist/ideal_outputs.png" /><media:content medium="image" url="https://dorianbrown.dev/assets/images/mnist/ideal_outputs.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Rube Goldberg Machine of Coding Languages</title><link href="https://dorianbrown.dev/befunge/" rel="alternate" type="text/html" title="Rube Goldberg Machine of Coding Languages" /><published>2018-04-05T00:00:00+02:00</published><updated>2018-04-05T00:00:00+02:00</updated><id>https://dorianbrown.dev/befunge</id><content type="html" xml:base="https://dorianbrown.dev/befunge/">&lt;h1 id=&quot;opening-remarks&quot;&gt;Opening Remarks&lt;/h1&gt;

&lt;p&gt;Recently I stumbled onto an article about the well-known &lt;a href=&quot;https://www.wikiwand.com/en/Brainfuck&quot;&gt;Brainfuck&lt;/a&gt; coding language, and I found it fascinating that someone would spend time and energy to create a totally impractical programming language.&lt;/p&gt;

&lt;p&gt;To quote the wikipedia page on befunge:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;… it is not intended for practical use, but to challenge and amuse programmers.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;After further reading I found out Brainfuck is what some people call an “esoteric programming language”. Although it was one of the first and most (in)famous, there are many more of these &lt;a href=&quot;https://www.wikiwand.com/en/Esoteric_programming_language&quot;&gt;wacky creations&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And while browsing through this list of monstrosities, I discovered the beautiful and weird language that is Befunge.&lt;/p&gt;

&lt;h1 id=&quot;hello-befunge&quot;&gt;Hello Befunge&lt;/h1&gt;

&lt;p&gt;Let’s get down to how Befunge works. The easiest way to explain how it’s run by an interpreter, is to show a working example. What way is better than a Hello World program:&lt;/p&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;/assets/images/befunge/hello_world1.gif&quot; style=&quot;width: 90%;&quot; /&gt;
    &lt;figcaption&gt; The stack is LIFO, so we need to reverse the text &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;There are a few things to notice here:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Code is interpreted by the orange cursor, which starts at the top left cell moving to the right. When it moves across a character, something happens.&lt;/li&gt;
  &lt;li&gt;A &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;&lt;/code&gt; character initiates a string, until the cursor encounters another one. Within a string any characters encountered are put onto a stack, letter by letter. These are used when a print statement is called.&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;,&lt;/code&gt; character is a kind of print or echo. It removes the first character in the stack, and prints it to the console.&lt;/li&gt;
  &lt;li&gt;Finally the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@&lt;/code&gt; character terminates the cursor, and ends the program.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Simple enough right? Now to bend your mind we’ll show you another, more creative, hello world:&lt;/p&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;/assets/images/befunge/hello_world2.gif&quot; style=&quot;width: 90%;&quot; /&gt;
    &lt;figcaption&gt; The stack is LIFO, so we need to reverse the text &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Try to import that into python! This is the core of befunge’s wackiness. You can actually steer your cursor using certain characters.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;gt;&lt;/code&gt;,&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v&lt;/code&gt;,&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;&lt;/code&gt;,&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;^&lt;/code&gt; characters can steer the cursor in each of the four possible directions.&lt;/li&gt;
  &lt;li&gt;A random direction can be given by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;?&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;|&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_&lt;/code&gt; characters pop from the stack, and move the cursor down/right if the popped value was 0, and up/left otherwise.&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p&lt;/code&gt; character can change the code of the program. It pops the first 3 values $(x_1,x_2,x_3) = (y,x,v)$ and uses these to set the character at coordinates $(x,y)$ to value $v$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There are &lt;a href=&quot;https://www.wikiwand.com/en/Befunge#/Befunge-93_instruction_list&quot;&gt;many more instructions&lt;/a&gt; which allow you to manipulate the stack in handy ways, but these are the ones that make Befunge a unique language.&lt;/p&gt;

&lt;p&gt;One last thing you might wonder, what happens when the cursor goes off the “grid”? What the creators of the language did, is gave that grid the topology of a torus. This basically means the left wraps to the right, and the top to the bottom. See an example of that here:&lt;/p&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;/assets/images/befunge/torus.gif&quot; style=&quot;width: 60%;&quot; /&gt;
    &lt;figcaption&gt; A torus is a donut, in case you forgot. &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;code-examples&quot;&gt;Code Examples&lt;/h1&gt;

&lt;p&gt;So with a short introduction to the basics, you can already write some interesting bits of code, which can do some simple things. The fun part about coding in befunge is that you have to get creative to accomplish tasks that would be simple in other languages. Here are some neat examples of befunge, either solving standard coding excercises or showcasing the cool things you can do with the language.&lt;/p&gt;

&lt;h2 id=&quot;looping-looping&quot;&gt;Looping Looping…&lt;/h2&gt;

&lt;p&gt;So what about things like loops? Well we can just construct that from the arrows and conditional arrows. The code below contains two loops, the inner loop to print out the string, and the outer loop to fill the stack with the string once it’s empty.&lt;/p&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;/assets/images/befunge/loop.gif&quot; style=&quot;width: 90%;&quot; /&gt;
    &lt;figcaption&gt; We don't need loops. We make loops. &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;We use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:&lt;/code&gt; symbol to double the first character on the stack, since the conditional &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_&lt;/code&gt; character pops a character when it checks. This way we just pop the character we added with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:&lt;/code&gt;, and we still print the full string.&lt;/p&gt;

&lt;h2 id=&quot;random-number-generator&quot;&gt;Random Number Generator&lt;/h2&gt;

&lt;p&gt;Another standard example is a random number generator, and in befunge it’s an extra cool one. We use a whole bunch of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;?&lt;/code&gt;’s to guide the cursor into one of the 9 strategically placed digits.&lt;/p&gt;

&lt;p&gt;Since the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;?&lt;/code&gt; character can send the cursor in any of the 4 directions, we need to be a little careful that we don’t let the cursor escape our construction. If it did escape every now and then, our generator probably wouldn’t be uniformly distributed. But befunge probably won’t be used for much production code. Lets hope.&lt;/p&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;/assets/images/befunge/rng.gif&quot; style=&quot;width: 90%;&quot; /&gt;
    &lt;figcaption&gt; We don't need loops. We make loops. &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;examples-from-reddit&quot;&gt;Examples from Reddit&lt;/h2&gt;

&lt;p&gt;After posting this article &lt;a href=&quot;https://www.reddit.com/r/programming/comments/8dz2g6/befunge_the_rube_goldberg_machine_of_coding/&quot;&gt;on reddit&lt;/a&gt;, I got some really cool examples of programs using befunge. Here’s a list of some of the cool ones:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.frox25.no-ip.org/%7Emtve/code/eso/bef/chess/&quot;&gt;Befunge Chess&lt;/a&gt;, a chessboard which is displayed in the “program space” and uses the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p&lt;/code&gt; operator to update the chessboard.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://de.wikipedia.org/wiki/Befunge?oldformat=true#BefBef&quot;&gt;BefBef&lt;/a&gt;, a befunge interpreter which is written in befunge. The “program space” of which is being interpreted is actually a subset of the interpreters “program space”. Really awesome!&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://de.wikipedia.org/wiki/Befunge?oldformat=true#Conways_Game_of_Life&quot;&gt;Conway’s Game of Life&lt;/a&gt;, because a programming language isn’t complete without an implementation of the &lt;a href=&quot;https://www.wikiwand.com/en/Conway%27s_Game_of_Life&quot;&gt;Game of Life&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;try-it-out&quot;&gt;Try it out&lt;/h1&gt;

&lt;p&gt;Finally I want to include a javascript interpreter, which you can use to try out the language and all the neat things shown above. I used this interpreter to create all the gifs shown in the post, and a larger version with a visible stack can be found on this &lt;a href=&quot;http://qiao.github.io/javascript-playground/visual-befunge93-interpreter/&quot;&gt;site&lt;/a&gt;. Shoutout to Xueqiao Xu for the website and for writing this interpreter in javascript for a really weird language.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Please share any cool programs you make in the comments, and I’ll see if I can update the post with them in the future!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;/assets/javascript/befunge.js&quot;&gt;&lt;/script&gt;

&lt;div id=&quot;main&quot; style=&quot;background-color: rgb(242, 243, 243); padding: 15px; border-radius: 5px; display: flex; flex-wrap: wrap;&quot;&gt;
    &lt;div style=&quot;flex: 0 0 65%; text-align: center;&quot;&gt;
        &lt;canvas id=&quot;canvas&quot; width=&quot;420&quot; height=&quot;150&quot; style=&quot;padding: 0px 0px 15px 0px;&quot;&gt;&lt;/canvas&gt;
        &lt;button id=&quot;initbutton&quot;&gt;Init&lt;/button&gt;
        &lt;button id=&quot;runbutton&quot;&gt;Run&lt;/button&gt;
        &lt;button id=&quot;stepbutton&quot;&gt;Step&lt;/button&gt;
        &lt;button id=&quot;stopbutton&quot;&gt;Stop&lt;/button&gt;
    &lt;/div&gt;
    &lt;div id=&quot;left&quot; style=&quot;flex: 0 0 35%; padding: 0px 5px 5px 5px;&quot;&gt;
        &lt;b&gt;Edit the Codes Below&lt;/b&gt;
        &lt;textarea id=&quot;codebox&quot;&gt;&lt;/textarea&gt;
        &lt;b&gt;Output&lt;/b&gt;
        &lt;textarea id=&quot;output&quot; readonly=&quot;true&quot;&gt;&lt;/textarea&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;script type=&quot;text/javascript&quot;&gt;
    init_func = window.onload
    window.onload = function() {
        init_func()
        codeBox = document.getElementById(&quot;codebox&quot;);
        codeBox.value = `&quot;dlroW olleH&quot;,,,,,,,,,,,@`
    }
&lt;/script&gt;</content><author><name>Dorian</name></author><summary type="html">And while browsing through this list of monstrosities, I discovered the beautiful and weird language that is Befunge.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://dorianbrown.dev/assets/images/befunge/befunge.png" /><media:content medium="image" url="https://dorianbrown.dev/assets/images/befunge/befunge.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Dockerizing databases</title><link href="https://dorianbrown.dev/dockerized-databases/" rel="alternate" type="text/html" title="Dockerizing databases" /><published>2018-03-20T00:00:00+01:00</published><updated>2018-03-20T00:00:00+01:00</updated><id>https://dorianbrown.dev/dockerized-databases</id><content type="html" xml:base="https://dorianbrown.dev/dockerized-databases/">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Running postgres in a container&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Connecting to that container&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Mounting volume seperately&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Creating a python script to connect to it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Tying it all together&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Dorian</name></author><summary type="html">Running postgres in a container</summary></entry><entry><title type="html">Publishing blog posts to Wordpress using RMarkdown</title><link href="https://dorianbrown.dev/publishing-blog-posts-to-wordpress-using-rrmarkdown/" rel="alternate" type="text/html" title="Publishing blog posts to Wordpress using RMarkdown" /><published>2017-11-23T00:00:00+01:00</published><updated>2017-11-23T00:00:00+01:00</updated><id>https://dorianbrown.dev/publishing-blog-posts-to-wordpress-using-rrmarkdown</id><content type="html" xml:base="https://dorianbrown.dev/publishing-blog-posts-to-wordpress-using-rrmarkdown/">&lt;h1 id=&quot;easy-posting&quot;&gt;Easy posting&lt;/h1&gt;

&lt;p&gt;When I was getting this blog setup, I decided I needed to make creating new posts as easy as possible. I want to use my daily commute (which is about 1.5 hours each day) to create new content, and in order to make the barrier as small as possible I started looking into the options for posting from some kind of notebook format.&lt;/p&gt;

&lt;p&gt;After a lot of researching the options, looking into the available libraries, trying things out, I arrived at the combination of &lt;strong&gt;Wordpress&lt;/strong&gt;, &lt;strong&gt;R Markdown&lt;/strong&gt;, and the awesome &lt;strong&gt;RWordpress&lt;/strong&gt; library.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;For those just interested in the code, the github repository for RWordpress can be found &lt;a href=&quot;https://github.com/duncantl/RWordPress&quot;&gt;here&lt;/a&gt;, and the script I use for posting my articles is on my github &lt;a href=&quot;https://github.com/dorianbrown/blog_posts/blob/master/publish_post.R&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So how can you use RWordpress to make your life easier? To start things off you need to install a few libraries from github.&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;monokai&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;install.packages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;devtools&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;devtools&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;install_github&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;duncantl/XMLRPC&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;install_github&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;duncantl/RWordPress&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;install.packages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;knitr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;XMLRPC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RWordPress&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;knitr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next step, write your blog post. I’m sure most of you are familiar with the 
RMarkdown format, but if not it’s basically a superset of standard markdown, which allows the execution of code within the document (check out the &lt;a href=&quot;http://rmarkdown.rstudio.com/&amp;quot;&quot;&gt;docs&lt;/a&gt;). Once you’ve got your draft ready, you can preview it with the preview/knit button in Rstudio.&lt;/p&gt;

&lt;p&gt;So, you’ve finished your post and you’re eager to share it with the world. Running this R script will do the trick:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;monokai&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;pw&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;readLines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;../data/wp_password.txt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WordpressLogin&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dsbrown&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WordpressURL&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://www.findingsignal.xyz/xmlrpc.php&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rmd_file&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Post thumbnail&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opts_knit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;upload.fun&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RWordPress&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uploadFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;})&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;knit2wp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rmd_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;categories&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;publish&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And just like that, your article is live on Wordpress and visible to all your readers! The script is available &lt;a href=&quot;https://github.com/dorianbrown/blog_posts/blob/master/publish_post.R&quot;&gt;here&lt;/a&gt;, and I hope this will make blogging easier for some of you.&lt;/p&gt;

&lt;p&gt;Happy posting!&lt;/p&gt;</content><author><name></name></author><summary type="html">When I was getting this blog setup, I decided I needed to make creating new posts as easy as possible. I want to use my daily commute (which is about 1.5 hours each day) to create new content, and in order to make the barrier as small as possible it started looking into the options for posting from some kind of notebook format.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://dorianbrown.dev/assets/images/wordpress.jpg" /><media:content medium="image" url="https://dorianbrown.dev/assets/images/wordpress.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">A Short Introduction</title><link href="https://dorianbrown.dev/a-short-introduction-and-what-this-blog-is-going-to-be-about/" rel="alternate" type="text/html" title="A Short Introduction" /><published>2017-10-08T00:00:00+02:00</published><updated>2017-10-08T00:00:00+02:00</updated><id>https://dorianbrown.dev/a-short-introduction-and-what-this-blog-is-going-to-be-about</id><content type="html" xml:base="https://dorianbrown.dev/a-short-introduction-and-what-this-blog-is-going-to-be-about/">&lt;h1 id=&quot;starting-things-off&quot;&gt;Starting things off&lt;/h1&gt;

&lt;p&gt;Well this is going to be my first post. As I’m sure lots of first time bloggers have run into, you feel a lot of pressure to make the first post amazing. You feel the need to start with a great first post, because you’ve told all your friends you’re going to make a blog, and some of them are eagerly awaiting that first post. And then pressure builds…&lt;/p&gt;

&lt;h1 id=&quot;data-fu&quot;&gt;Data-Fu&lt;/h1&gt;

&lt;p&gt;Let me give a brief introduction of who I am. My name is Dorian Brown, and for about a year now I get to officially call myself a data scientist (atleast that’s the title at my company). I’ve been living in the Netherlands for about 10 years in a small student town called Leiden near Amsterdam. It’s worth a visit for a day if you’re ever in the area. I studied theoretical mathematics here which I enjoyed a lot, and focused mostly on probablilty, statistics, optimization and algorithms.&lt;/p&gt;

&lt;p&gt;After finishing my master’s degree, I ran into the problem I’m sure many of the people reading this will have also run into: what do I want to do with my life? Unsurprisingly, I’m still trying to figure that out, but luckily I’ve made a little progress in the last few years. My first job was with an engineering firm which is contracted by companies which want a pipeline laid from A to B, usually over a sea floor. The “get down to business” mentality was great, and I really enjoyed the engineering culture there, but I quickly realised that I was missing a lot of experience in data-fu, and if I wanted to get better at that this wasn’t the best place to learn.&lt;/p&gt;

&lt;p&gt;After a year I got a job at a dutch bank as a data scientist, and so far I’ve been able to learn a lot. Traditionally banks have been relatively old-fashioned, but in recent years they’ve been feeling the pressure of so-called fintech start-ups. This competition has caused them to try and modernize, which is very encouraging to see.&lt;/p&gt;

&lt;h1 id=&quot;what-this-blog-is-about&quot;&gt;What this blog is about&lt;/h1&gt;

&lt;p&gt;So enough about me, you’re probably more interested in what kind of content you can expect to find here. Although the overarching theme is going to be data-science and all the stuff related to it, I’ll try and organize content in the following categories.&lt;/p&gt;

&lt;h2 id=&quot;analyses&quot;&gt;Analyses&lt;/h2&gt;

&lt;p&gt;I’ll start these posts out with a question and a dataset, and at the end of the post I’ll post a conclusion (with an answer to the original question if we’re lucky). For all of these the code will be available on my github account, and in the post I’ll try and show the more interesting bits of analysis/code, and keep the uninteresting stuff hidden.&lt;/p&gt;

&lt;h2 id=&quot;technical-stuff&quot;&gt;Technical stuff&lt;/h2&gt;

&lt;p&gt;I’ve realised that I love the technical stuff about the data science profession. I realise it’s a pitfall to concentrate to much on the tools we use, and not enough on the actual question and the “real world” stuff, but I guess I’m a bit of an engineer myself. I often have discussion with colleagues on stuff like code formatting, R vs Python, new developments in software tooling, etc. I’ll try and share some of that stuff here too!&lt;/p&gt;

&lt;h2 id=&quot;mathematics&quot;&gt;Mathematics&lt;/h2&gt;

&lt;p&gt;Although lots of the really theoretical stuff I learned during my studies has eroded from my brain, it still does help a lot with the work we do. I’m not going to be publishing my own papers any time soon, but I like to read about new developments and things relevant to the data science community.&lt;/p&gt;

&lt;h2 id=&quot;personal-experiences&quot;&gt;Personal experiences&lt;/h2&gt;

&lt;p&gt;I’ve noticed that since the data science profession is new and somewhat undefined, lots of people in this field run into problems within their organizations. This can range from things like unrealistic expectation (data scientists are the magic bullets that can solve all our problems!), what work falls within a data scientists job description, how to manage a team of data scientists, etc. I think if we all try and share the kind of things we run into, and what some good approaches to solving these problems are, it will help us all be better at our work.&lt;/p&gt;

&lt;h1 id=&quot;rounding-things-off&quot;&gt;Rounding things off&lt;/h1&gt;

&lt;p&gt;I hope you guys managed to keep reading this far in. This was my way to conquer the whole writers block, and to give my a blog a little structure to help the posting start, and to let you guys know what to expect. Please feel free to reply to any posts and leave feedback. I’d love for discussion to start regarding any posts, and feedback is always welcome!&lt;/p&gt;</content><author><name></name></author><summary type="html">Well this is going to be my first post. As I'm sure lots of first time bloggers have run into, you feel a lot of pressure to make the first post amazing. You feel the need to start with a great first post, because you've told all your friends you're going to make a blog, and some of them are eagerly awaiting that first post. And then pressure builds…</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://dorianbrown.dev/assets/images/solitaire.jpg" /><media:content medium="image" url="https://dorianbrown.dev/assets/images/solitaire.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>