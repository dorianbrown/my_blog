<!DOCTYPE html>
<html>
<head>
  <title>Understanding Tensorflow with MNIST – Dorian Brown – Finding signal and escaping the noise</title>

      <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="During a hackathon at work I finally had tensorflow and it's api "click" for me. This article shares some of what made it click for me, in addition to the other stuff I did trying to understand the weights of the trained network better." />
    <meta property="og:description" content="During a hackathon at work I finally had tensorflow and it's api "click" for me. This article shares some of what made it click for me, in addition to the other stuff I did trying to understand the weights of the trained network better." />
    
    <meta name="author" content="Dorian Brown" />

    
    <meta property="og:title" content="Understanding Tensorflow with MNIST" />
    <meta property="twitter:title" content="Understanding Tensorflow with MNIST" />
    

  <!--[if lt IE 9]>
<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->

<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.2/css/bootstrap.min.css"
      integrity="sha384-Smlep5jCw/wG7hdkwQ/Z5nLIefveQRIY9nfy6xoR1uRYBtpZgI6339F5dgvm/e9B" crossorigin="anonymous">
<!-- JQuery latest -->
<script src="//code.jquery.com/jquery-latest.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
        integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
        crossorigin="anonymous"></script>
<!-- Bootstrap -->
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.2/js/bootstrap.min.js"
        integrity="sha384-o+RDsa0aLu++PJvFqy8fFScvbHFLtbvScb8AjopnFD+iEQ7wo/CG0xlczd+2O/em"
        crossorigin="anonymous"></script>
<!-- add after bootstrap.min.js -->
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.0/dist/bootstrap-toc.min.js"></script>
<link rel="alternate" type="application/rss+xml" title="Dorian Brown - Finding signal and escaping the noise"
      href="/feed.xml"/>
<link rel="shortcut icon" type="image/x-icon" href="/favicon.ico">
<!--Custom CSS>-->
<link rel="stylesheet" type="text/css" href="/style.css"/>
<link href="https://use.fontawesome.com/releases/v5.0.6/css/all.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
        type="text/javascript"></script>
<!-- Photo Gallery -->
<link href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe.min.css" rel="stylesheet">
<link href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/default-skin/default-skin.min.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe.min.js" type="text/javascript"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe-ui-default.js" type="text/javascript"></script>
<script src="/assets/javascript/jqPhotoSwipe.min.js"></script>
<!-- Fonts Import -->
<link href="https://fonts.googleapis.com/css?family=PT+Serif" rel="stylesheet"> 
<link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet"> 
<!-- Mathjax Stuff -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)'], ['$$','$$']]}});
</script>
<script type="text/javascript">
    (function($, window) {
      var adjustAnchor = function() {
          var $anchor = $(':target'),
                  fixedElementHeight = 125;
          if ($anchor.length > 0) {
            window.scrollTo(0, $anchor.offset().top - fixedElementHeight);
          }
      };
      $(window).on('hashchange load', function() {
          adjustAnchor();
      });
  })(jQuery, window);

<!-- Hiding header on scroll -->
</script>
<script src="/assets/javascript/headroom.min.js"></script>

  
  <!-- Stub here for jekyll-seo-tag plugin -->
  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Understanding Tensorflow with MNIST | Dorian Brown</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Understanding Tensorflow with MNIST" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="During a hackathon at work I finally had tensorflow and it’s api “click” for me. This article shares some of what made it click for me, in addition to the other stuff I did trying to understand the weights of the trained network better." />
<meta property="og:description" content="During a hackathon at work I finally had tensorflow and it’s api “click” for me. This article shares some of what made it click for me, in addition to the other stuff I did trying to understand the weights of the trained network better." />
<link rel="canonical" href="https://dorianbrown.dev/mnist-tensorflow/" />
<meta property="og:url" content="https://dorianbrown.dev/mnist-tensorflow/" />
<meta property="og:site_name" content="Dorian Brown" />
<meta property="og:image" content="https://dorianbrown.dev/assets/images/mnist/ideal_outputs.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-10-07T00:00:00+02:00" />
<script type="application/ld+json">
{"image":"https://dorianbrown.dev/assets/images/mnist/ideal_outputs.png","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://dorianbrown.dev/mnist-tensorflow/"},"url":"https://dorianbrown.dev/mnist-tensorflow/","headline":"Understanding Tensorflow with MNIST","dateModified":"2018-10-07T00:00:00+02:00","datePublished":"2018-10-07T00:00:00+02:00","description":"During a hackathon at work I finally had tensorflow and it’s api “click” for me. This article shares some of what made it click for me, in addition to the other stuff I did trying to understand the weights of the trained network better.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>

<body data-spy="scroll" data-target="#toc" data-offset="140">
<div class="wrapper-masthead">
  <header class="masthead clearfix">
    <a href="/" class="site-avatar"><img src="/assets/images/logo.png"></a>

    <div class="site-info">
      <h1 data-toc-skip class="site-name"><a href="/">Dorian Brown</a></h1>
      <p class="site-description">Finding signal and escaping the noise</p>
    </div>

    <nav>
      <a href="/">Blog</a>
      <a href="/categories/">Categories</a>
      <a href="/photography/">Photography</a>
      <a href="/about">About</a>
      <button class="btn btn-light" id="subscribe-button">Subscribe</button>
    </nav>
  </header>
</div>

<div id="not-header">
  <div class="container">
    <div class="row">
  <div class="col-lg-2">
    
      <nav id="toc" data-toggle="toc" class="sticky-top"></nav>
    
    
  </div>
  <div id="content" class="mx-auto col-lg-8">
    <article class="post">
  
  <figure class="title-figure">
    <img class="title-img" src="/assets/images/mnist/ideal_outputs.png">
    <div class="title-text"> Understanding Tensorflow with MNIST </div>
  </figure>
  

  <div class="date">
    October  7, 2018
  </div>

  <div class="post-categories">
    <b>Categories: </b>
    <br>
    
    
    <a href="/categories/#Machine%20Learning">Machine Learning</a>
     <br> 
    
    <a href="/categories/#Programming">Programming</a>
    
    
  </div>

  <div class="entry">
    <h1 id="opening-remarks">Opening Remarks</h1>

<p>Recently at work we had a hackathon on tensorflow, which is something I’d tried (running examples) but never really took the time to play with it. I’d like to share my experiences with the tensorflow computational graph, and some of the flexibility it has to do cool stuff.</p>

<blockquote>
  <p>I’m not a tensorflow expert, so please let me know if there are any inaccuracies I need to correct <img class="emoji" title=":grin:" alt=":grin:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f601.png" height="20" width="20"></p>
</blockquote>

<h1 id="quick-introduction-to-tensorflow">Quick Introduction to Tensorflow</h1>

<h2 id="the-computation-graph">The computation graph</h2>

<p>In order to do anything in tensorflow, you need to create a so called computation graph. This is the main construct used to break things down into problems your GPU can efficiently solve. Like Numpy and PySpark, python is just a wrapper for telling other engines to run your computations. In the case of tensorflow, this is for CUDA and your GPU.</p>

<p>Sending data from your GPU to python and back creates a lot of performance overhead, so the computation graph is a way of helping you do that as little as possible.</p>

<figure style="text-align: center;">
    <img src="/assets/images/mnist/comp_graph.gif" style="width: 75%;">
    <figcaption> Example of a tensorflow computation graph</figcaption>
</figure>

<p>You define a computation graph using special tensor objects (eg. constants, vectors, matrices) and tensor operations (eg. multiply, weighted sum, apply loss function). Inputs to your graph (like a training dataset) are defined as placeholders.</p>

<h2 id="calculating-a-matrix-inverse-the-hard-way">Calculating a matrix inverse, the hard way</h2>

<p>Say we want to make a simple computation graph, but a little more complicated than adding two numbers. Let’s take a square matrix as input, and multiple it by some set matrix $A$. Our goal is to create a graph that finds the matrix inverse $A^{-1}$, which we enforce with the loss. This function tries to make the output as close to the identity matrix as possible.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="monokai"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># We'll put this into the graph later
</span><span class="n">rand_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="c1"># We start making our graph here
</span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"A"</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">constant</span><span class="p">(</span><span class="n">rand_matrix</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
<span class="n">id_mat</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">identity</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">output</span> <span class="o">-</span> <span class="n">id_mat</span><span class="p">))</span>

<span class="c1"># Here we define some "meta-graph" stuff
</span><span class="n">optimiser</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">AdagradOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">).</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="n">init_op</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
</code></pre></div></div>

<p>So in the above section we defined:</p>
<ul>
  <li>some variables (stuff which get updated and persists between runs)</li>
  <li>some constants which don’t change</li>
  <li>matrix multiplication, which transforms our input into an output in our case</li>
  <li>a loss function</li>
  <li>an optimiser which changes the variables in the graph to minimize the loss</li>
  <li>a variable initializer, which creates initial states for any variables which are defined using a distribution (we don’t use those)</li>
</ul>

<p>With this defined, we can start training the model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="monokai"><code><span class="c1"># Number of passes over the data
</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">100_000</span>

<span class="c1"># Context managers work nicely with tensorflow sessions
</span><span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">init_op</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">epoch_loss</span> <span class="o">=</span> <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">([</span><span class="n">optimiser</span><span class="p">,</span> <span class="n">loss</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">e</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="n">epoch_loss</span><span class="p">)</span>

    <span class="c1"># Lets evaluate the learned matrix A
</span>    <span class="n">x_</span> <span class="o">=</span> <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">x_</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">rand_matrix</span><span class="p">))</span>
</code></pre></div></div>

<p>Running the optimizer once will update the variable x by one step. In order to get a good estimate, we run the optimizer 100,000 times. With the last bit of code, we extract the learned matrix x from the tf session and check if it is indeed close to the inverse.</p>

<p>In order to see if the algorithm is converging we can look at the loss of the model vs the training epoch. Just to be sure we also check to see if the learned matrix x actually does approximate $A^{-1}$ (it does).</p>

<figure style="text-align: center;">
    <img src="/assets/images/mnist/example_learning_rate.png" style="width: 75%;">
    <figcaption>Yay, our loss is going down!</figcaption>
</figure>

<p>We trained our first tensorflow model! This little excercise helped me understand the concepts of the computation graph, and it should make the later example easier to follow if you’re new to tensorflow.</p>

<h1 id="classifying-mnist-with-tensorflow">Classifying MNIST with tensorflow</h1>

<p>Now that we understand the basics of tensorflow, lets build single-layered neural network using tensorflow to classify these images.</p>

<h2 id="model-definition">Model Definition</h2>

<p>We’ll rescale the images to 28x28 pixel images and represent these by a 28x28 matrix, where each matrix entry represents the pixels color-intensity with a number between 0 and 1.</p>

<p>The output of the network will be a one-hot encoding of the numbers 0-9, so each one will be a 10-dimensional vector.</p>

<p>Finally, the network connecting the inputs to the outputs will be a single densely connected layer, a relu activation function, and a softmax to ensure we are left with a probability vector as the output.</p>

<figure style="text-align: center;">
    <img src="/assets/images/mnist/network_diagram.svg" style="width: 75%;">
    <figcaption>The network architecture of our MNIST classifier</figcaption>
</figure>

<h2 id="implementation">Implementation</h2>

<p>Just like the previous example, we’re going to define our network in a computation graph, using tensors and tensor operations. We’ll use a placeholder x for our input images and y for the one-hot encoded label. Note that we reshaped the images from a 28x28 matrix to a vector of length 784.</p>

<p>Here’s the graph we used for this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="monokai"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.examples.tutorials.mnist</span> <span class="kn">import</span> <span class="n">input_data</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Read data
</span><span class="n">mnist</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">.</span><span class="n">read_data_sets</span><span class="p">(</span><span class="s">"MNIST_data/"</span><span class="p">,</span> <span class="n">one_hot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Python optimisation variables
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># declare the training data placeholders
# input x - for 28 x 28 pixels = 784
</span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">784</span><span class="p">])</span>
<span class="c1"># now declare the output data placeholder - 10 digits
</span><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>

<span class="c1"># now declare the weights connecting the input to the hidden layer
</span><span class="n">W1</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">random_normal</span><span class="p">([</span><span class="mi">784</span><span class="p">,</span> <span class="mi">300</span><span class="p">],</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">0.03</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s">'W1'</span><span class="p">)</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">random_normal</span><span class="p">([</span><span class="mi">300</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s">'b1'</span><span class="p">)</span>
<span class="n">W2</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">random_normal</span><span class="p">([</span><span class="mi">300</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">0.03</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s">'W2'</span><span class="p">)</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">random_normal</span><span class="p">([</span><span class="mi">10</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s">'b2'</span><span class="p">)</span>

<span class="c1"># calculate the output of the hidden layer
</span><span class="n">hidden_out</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W1</span><span class="p">),</span> <span class="n">b1</span><span class="p">)</span>
<span class="n">hidden_out</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">hidden_out</span><span class="p">)</span>
<span class="n">y_</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">hidden_out</span><span class="p">,</span> <span class="n">W2</span><span class="p">),</span> <span class="n">b2</span><span class="p">))</span>
<span class="n">y_clipped</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">y_</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mf">0.9999999</span><span class="p">)</span>
<span class="n">cross_entropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_clipped</span><span class="p">)</span>
                         <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_clipped</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># add an optimiser
</span><span class="n">optimiser</span> <span class="o">=</span> <span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
                <span class="p">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">))</span>

<span class="c1"># finally setup the initialisation operator
</span><span class="n">init_op</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

<span class="c1"># define an accuracy assessment operation
</span><span class="n">correct_prediction</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">equal</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct_prediction</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">))</span>

<span class="c1"># Training the network
</span><span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">init_op</span><span class="p">)</span>
    <span class="n">total_batch</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mnist</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">labels</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">avg_cost</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_batch</span><span class="p">):</span>
            <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span> <span class="o">=</span> <span class="n">mnist</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">next_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">([</span><span class="n">optimiser</span><span class="p">,</span> <span class="n">cross_entropy</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">batch_x</span><span class="p">,</span> 
                                                                   <span class="n">y</span><span class="p">:</span> <span class="n">batch_y</span><span class="p">})</span>
            <span class="n">avg_cost</span> <span class="o">+=</span> <span class="n">c</span> <span class="o">/</span> <span class="n">total_batch</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Epoch:"</span><span class="p">,</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="s">"cost ="</span><span class="p">,</span> <span class="s">"{:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">avg_cost</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">mnist</span><span class="p">.</span><span class="n">test</span><span class="p">.</span><span class="n">images</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">mnist</span><span class="p">.</span><span class="n">test</span><span class="p">.</span><span class="n">labels</span><span class="p">}))</span>
</code></pre></div></div>

<p>Let’s check out the output of this model, and see if it did what we wanted to. We’ll look at the cross-entropy loss on the training set, test set, and the overall accuracy on the test set, for each epoch.</p>

<figure style="text-align: center;">
    <img src="/assets/images/mnist/mnist_output.png" style="width: 75%;">
    <figcaption>How our network learns over time </figcaption>
</figure>

<p>This will learn the network weights $W_1, b_1, W_2, b_2$ and give a pretty decent accuracy for classification, 98% on the test set!</p>

<h1 id="looking-into-our-network">Looking into our network</h1>

<h2 id="image-fingerprints">Image fingerprints</h2>

<p>As this was my first time working with tensorflow and a neural network, I was interested in dissecting the network a bit. The input and output layers aren’t too interesting, so I thought I’d look at the hidden layer for out-of-sample images.</p>

<p>Getting each hidden layer was quite simple. If you want the hidden layers for all 0 images:</p>
<ul>
  <li>Create a tf-session and train the neural network. Don’t close the session!</li>
  <li>Filter the test images to only the 0 ones.</li>
  <li>Running <code class="language-plaintext highlighter-rouge">sess.run(hidden_out, feed_dict={x: input_data})</code> will return a tensor where each row is the hidden layer for the images that you fed it.</li>
</ul>

<p>If you plot this matrix for each label, we get the following image:</p>

<figure style="text-align: center;">
    <img src="/assets/images/mnist/hidden_layers.png" style="width: 100%;">
    <figcaption>Showing trained neural network hidden layer vectors for each test image</figcaption>
</figure>

<p>So what do we see here? One thing that is pretty clear, is that each class shows clear horizontal bands across all the test images. Each vertical stripe is the hidden layer vector for that specific test set image. If we see a horizontal stripe, that means that vector entry is “activated” in most of the images with that label.</p>

<p>This unique pattern of bands is what the second bit of linear algebra ($A_2\cdot x + b_2$) uses to classify these images, as the combination is a kind of unique indentifier or fingerprint for each image. Our network learned this itself, which is super cool!</p>

<h2 id="estimating-optimal-input-images">Estimating optimal input images</h2>

<p>After seeing the patterns in the hidden layers, I was curious what the “otpimal” image would be for each class. The first approach I tried wasn’t very feasible, reversing the network direction to give an input for a specific output. This has to do with the fact that we reduce a $784 \times 1$ vector to a $10 \times 1$ vector, so a lot of information is lost.</p>

<p>There’s another approach we can use, where we train another model which tries to learn the input vector which best outputs one of the outputs corresponding with the one-hot encoded labels. So we try to find the best image input for every output number, according to the network, or technically we’re finding the image vector $\hat{V}_i$ which minimizes the cross-entropy loss between it’s output $\hat{y}_i$ and the unit vector $e_i$ for every $i =0,1,\cdots,9$. The code for this can be found <a href="https://github.com/dorianbrown/notebooks/blob/master/mnist_nn.ipynb">here</a>.</p>

<p>Here are the first images I got with this approach. The images on the top are the average pixel density of all the test sampes, and on the bottom are the “ideal images” according to the neural network.</p>

<figure style="text-align: center;">
    <img src="/assets/images/mnist/nonideal_outputs.png" style="width: 100%;">
    <figcaption>This looks like it might be overfitting...</figcaption>
</figure>

<h1 id="regularization-to-the-rescue">Regularization to the rescue</h1>

<p>So it seems that since our input image has 784 free parameters and we’re trying to create a vector of length 10. Our model is seriously overfitting.</p>

<p>I tried regularization (I wasn’t sure what else to do) and started out with the L1 version. That killed most of the pixels except for a few. L2, on the other hand, was a little more gentle and worked like a charm! Adding a <code class="language-plaintext highlighter-rouge">penalty*tf.reduce_sum(x**2)</code> to the cross-entropy term allows us to add a little bias and reduce the variance, giving this much improved version.</p>

<p>L2 regularization with a penalty of 0.25 gave these optimal images</p>

<figure style="text-align: center;">
    <img src="/assets/images/mnist/ideal_outputs.png" style="width: 100%;">
    <figcaption>Yay, signal in the noise!</figcaption>
</figure>

<p>So we finally get some patterns in our ideal images. In each of the images we can roughly make out each of the numbers, but there are some strange irregularities in them.</p>

<p>These irregularities might be the pixels the network finds important for distinguishing numbers, but maybe they’re artifacts from the regularization. Pretty neat though, as there are clearly patterns in the unique areas of each number.</p>

<h1 id="the-end">The end</h1>

<p>For those of you who held out until the end, here’s a <img class="emoji" title=":doughnut:" alt=":doughnut:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f369.png" height="20" width="20">. I hope you either read something interesting or learned something new along the way.</p>

<p>I’m still new to the whole ML-blogging, so if you’ve got any reactions or tips please leave a message in the comments. You would really help me make the blog better!</p>

  </div>

  <hr style="margin-top: 3rem;">

  <h3 data-toc-skip>Sharing is caring.</h3>
  <div class="a2a_kit a2a_kit_size_30 a2a_default_style" style="margin: 0rem 0rem;">
  <a class="a2a_button_twitter a2a_counter"></a>
  <a class="a2a_button_linkedin a2a_counter"></a>
  <a class="a2a_button_hacker_news a2a_counter"></a>
  <a class="a2a_button_whatsapp a2a_counter"></a>
  <a class="a2a_button_reddit a2a_counter"></a>
  <a class="a2a_button_facebook a2a_counter"></a>
  </div>

  <div class="wrapper">
  <h3 data-toc-skip> Did you like the article? Subscribe for more.</h3>
      <form action="https://dev.us20.list-manage.com/subscribe/post-json?u=fb1deb852e0da99410d8eb6b2&amp;id=8acc3cf1bd&amp;c=?" method="get" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate">
          <input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="Email Address here" required>
          <div style="position: absolute; left: -5000px;" aria-hidden="true">
              <input type="text" name="b_e44c1f194bec93e238615469e_f6f826e769" tabindex="-1" value="">
          </div>
          <input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="btn btn-secondary">
          <div id="subscribe-result">
          </div>
      </form>
</div>

<script>
  $(document).ready(function () {
    var $form = $('#mc-embedded-subscribe-form')
    if ($form.length > 0) {
      $('form input[type="submit"]').bind('click', function (event) {
        if (event) event.preventDefault()
        register($form)
      })
    }
  })

  function register($form) {
    $('#mc-embedded-subscribe').val('Sending...');
    $.ajax({
      type: $form.attr('method'),
      url: $form.attr('action'),
      data: $form.serialize(),
      cache: false,
      dataType: 'json',
      contentType: 'application/json; charset=utf-8',
      error: function (err) { alert('Could not connect to the registration server. Please try again later.') },
      success: function (data) {
        $('#mc-embedded-subscribe').val('Subscribe')
        if (data.result === 'success') {
          // Yeahhhh Success
          console.log(data.msg)
          $('#mc-embedded-subscribe').removeClass("btn-secondary").addClass("btn-success");
          $('#mc-embedded-subscribe').val('Success!')
          $('#mce-EMAIL').css('borderColor', '#ffffff')
          $('#subscribe-result').css('color', 'rgb(53, 114, 210)')
          $('#subscribe-result').html('<p>Thank you for subscribing.</p>')
          $('#mce-EMAIL').val('')
        } else {
          // Something went wrong, do something to notify the user.
          console.log(data.msg)
          $('#mce-EMAIL').css('borderColor', '#ff8282')
          $('#subscribe-result').css('color', '#ff8282')
          $('#subscribe-result').html('<p>' + data.msg + '</p>')
        }
      }
    })
  };
</script>


  <script async src="https://static.addtoany.com/menu/page.js"></script>
  <script>
    var a2a_config = a2a_config || {};
    a2a_config.counts = { recover_domain: 'dorianbrown.github.io' };
  </script>

</article>

  </div>
</div>

  </div>

  <div class="wrapper-footer">
    <div class="container">
      <footer class="footer">
        <a href="mailto:dorianstuartbrown@gmail.com"><i class="svg-icon email"></i></a>
<a href="https://github.com/dorianbrown"><i class="svg-icon github"></i></a>
<a href="https://www.linkedin.com/in/doriansbrown"><i class="svg-icon linkedin"></i></a>
<a href="http://stackoverflow.com/users/1415371/ballzoffury?tab=profile"><i class="svg-icon stackoverflow"></i></a>

      </footer>
    </div>
  </div>
</div>

<!-- Lightgallery call -->
<script type="text/javascript">
  $(document).ready(function () {
    $(function(){

      $('a').hover(function(e){

          $(this).attr('data-title', $(this).attr('title'));
          $(this).removeAttr('title');

      },
      function(e){

          $(this).attr('title', $(this).attr('data-title'));

      });
    });
    //By default, plugin uses `data-fancybox-group` attribute to create galleries.
    $(".fancybox").jqPhotoSwipe({
      galleryOpen: function (gallery) {
        //with `gallery` object you can access all methods and properties described here http://photoswipe.com/documentation/api.html
        //console.log(gallery);
        //console.log(gallery.currItem);
        //console.log(gallery.getCurrentIndex());
        //gallery.zoomTo(1, {x:gallery.viewportSize.x/2,y:gallery.viewportSize.y/2}, 500);
        gallery.toggleDesktopZoom();
      }
    });
    //This option forces plugin to create a single gallery and ignores `data-fancybox-group` attribute.
    $(".forcedgallery > a").jqPhotoSwipe({
      forceSingleGallery: true
    });
  });
</script>
  
<script type="text/javascript">
  $(document).ready(function(){
    $(".wrapper-masthead").each(function(){
      var headroom = new Headroom(this);
      headroom.init();
    })
  })
</script>

<script type="text/javascript">
  $(document).ready(function () {
  });
</script>

<script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script>
<script type="text/javascript">
  function showSubscribeModal() {
    window.dojoRequire(["mojo/signup-forms/Loader"], 
      function(L) { 
        L.start({
          "baseUrl":"mc.us20.list-manage.com",
          "uuid":"fb1deb852e0da99410d8eb6b2",
          "lid":"8acc3cf1bd",
          "uniqueMethods":true
      }) 
      document.cookie = 'MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC;';
      document.cookie = 'MCPopupSubscribed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC;';
    })
  }

  $('#subscribe-button').click(showSubscribeModal)
</script>



	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-107934583-2', 'auto');
		ga('send', 'pageview', {
		  'page': '/mnist-tensorflow/',
		  'title': 'Understanding Tensorflow with MNIST'
		});
	</script>
	<!-- End Google Analytics -->



</body>
</html>
