<!DOCTYPE html>
<html>
<head>
  <title>What is Supervised Learning? A Mathematical Perspective – Dorian Brown – Finding signal and escaping the noise</title>

      <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="This post is the first of a series of posts that serve as an introduction to the field of Machine Learning for those with a mathematical background. We'll start here by introducing features, labels, hypothesis spaces, loss functions and model generalization." />
    <meta property="og:description" content="This post is the first of a series of posts that serve as an introduction to the field of Machine Learning for those with a mathematical background. We'll start here by introducing features, labels, hypothesis spaces, loss functions and model generalization." />
    
    <meta name="author" content="Dorian Brown" />

    
    <meta property="og:title" content="What is Supervised Learning? A Mathematical Perspective" />
    <meta property="twitter:title" content="What is Supervised Learning? A Mathematical Perspective" />
    

  <!--[if lt IE 9]>
<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->

<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.2/css/bootstrap.min.css"
      integrity="sha384-Smlep5jCw/wG7hdkwQ/Z5nLIefveQRIY9nfy6xoR1uRYBtpZgI6339F5dgvm/e9B" crossorigin="anonymous">
<!-- JQuery latest -->
<script src="//code.jquery.com/jquery-latest.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
        integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
        crossorigin="anonymous"></script>
<!-- Bootstrap -->
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.2/js/bootstrap.min.js"
        integrity="sha384-o+RDsa0aLu++PJvFqy8fFScvbHFLtbvScb8AjopnFD+iEQ7wo/CG0xlczd+2O/em"
        crossorigin="anonymous"></script>
<!-- add after bootstrap.min.js -->
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.0/dist/bootstrap-toc.min.js"></script>
<link rel="alternate" type="application/rss+xml" title="Dorian Brown - Finding signal and escaping the noise"
      href="/feed.xml"/>
<link rel="shortcut icon" type="image/x-icon" href="/favicon.ico">
<!--Custom CSS>-->
<link rel="stylesheet" type="text/css" href="/style.css"/>
<link href="https://use.fontawesome.com/releases/v5.0.6/css/all.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
        type="text/javascript"></script>
<!-- Photo Gallery -->
<link href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe.min.css" rel="stylesheet">
<link href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/default-skin/default-skin.min.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe.min.js" type="text/javascript"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe-ui-default.js" type="text/javascript"></script>
<script src="/assets/javascript/jqPhotoSwipe.min.js"></script>
<!-- Fonts Import -->
<link href="https://fonts.googleapis.com/css?family=PT+Serif" rel="stylesheet"> 
<link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet"> 
<!-- Mathjax Stuff -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)'], ['$$','$$']]}});
</script>
<script type="text/javascript">
    (function($, window) {
      var adjustAnchor = function() {
          var $anchor = $(':target'),
                  fixedElementHeight = 125;
          if ($anchor.length > 0) {
            window.scrollTo(0, $anchor.offset().top - fixedElementHeight);
          }
      };
      $(window).on('hashchange load', function() {
          adjustAnchor();
      });
  })(jQuery, window);

<!-- Hiding header on scroll -->
</script>
<script src="/assets/javascript/headroom.min.js"></script>

  
  <!-- Stub here for jekyll-seo-tag plugin -->
  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>What is Supervised Learning? A Mathematical Perspective | Dorian Brown</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="What is Supervised Learning? A Mathematical Perspective" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This post is the first of a series of posts that serve as an introduction to the field of Machine Learning for those with a mathematical background. We’ll start here by introducing features, labels, hypothesis spaces, loss functions and model generalization." />
<meta property="og:description" content="This post is the first of a series of posts that serve as an introduction to the field of Machine Learning for those with a mathematical background. We’ll start here by introducing features, labels, hypothesis spaces, loss functions and model generalization." />
<link rel="canonical" href="https://dorianbrown.dev/what-is-supervised-learning/" />
<meta property="og:url" content="https://dorianbrown.dev/what-is-supervised-learning/" />
<meta property="og:site_name" content="Dorian Brown" />
<meta property="og:image" content="https://dorianbrown.dev/assets/images/ml_course/supervised_header.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-09-03T00:00:00+02:00" />
<script type="application/ld+json">
{"image":"https://dorianbrown.dev/assets/images/ml_course/supervised_header.jpg","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://dorianbrown.dev/what-is-supervised-learning/"},"url":"https://dorianbrown.dev/what-is-supervised-learning/","headline":"What is Supervised Learning? A Mathematical Perspective","dateModified":"2019-09-03T00:00:00+02:00","datePublished":"2019-09-03T00:00:00+02:00","description":"This post is the first of a series of posts that serve as an introduction to the field of Machine Learning for those with a mathematical background. We’ll start here by introducing features, labels, hypothesis spaces, loss functions and model generalization.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>

<body data-spy="scroll" data-target="#toc" data-offset="140">
<div class="wrapper-masthead">
  <header class="masthead clearfix">
    <a href="/" class="site-avatar"><img src="/assets/images/logo.png"/></a>

    <div class="site-info">
      <h1 data-toc-skip class="site-name"><a href="/">Dorian Brown</a></h1>
      <p class="site-description">Finding signal and escaping the noise</p>
    </div>

    <nav>
      <a href="/">Blog</a>
      <a href="/categories/">Categories</a>
      <a href="/photography/">Photography</a>
      <a href="/about">About</a>
      <button class="btn btn-light" id="subscribe-button">Subscribe</button>
    </nav>
  </header>
</div>

<div id="not-header">
  <div class="container">
    <div class="row">
  <div class="col-lg-2">
    
      <nav id="toc" data-toggle="toc" class="sticky-top"></nav>
    
    
  </div>
  <div id="content" class="mx-auto col-lg-8">
    <article class="post">
  
  <figure class="title-figure">
    <img class="title-img" src="/assets/images/ml_course/supervised_header.jpg">
    <div class="title-text"> What is Supervised Learning? A Mathematical Perspective </div>
  </figure>
  

  <div class="date">
    September  3, 2019
  </div>

  <div class="post-categories">
    <b>Categories: </b>
    <br>
    
    
    <a href="/categories/#Machine Learning">Machine Learning</a>
     <br> 
    
    <a href="/categories/#Mathematics">Mathematics</a>
    
    
  </div>

  <div class="entry">
    <blockquote>
  <p>This post is the first of a series of posts that serve as an introduction to the field of Machine Learning for those with a mathematical background. They’ve been based off of the Cornell “Machine Learning for Intelligent Systems” course, which has generously put <a href="https://www.youtube.com/playlist?list=PLl8OlHZGYOQ7bkVbuRthEsaLr7bONzbXS">all the classes on youtube</a>. My thanks to Kilian Weinberger for doing a great job teaching this course and for finding a perfect balance of theory and application.</p>

  <p>We assume some basic knowledge of linear algebra, probability theory, and optimization theory. I’ll try and include links and explanations with the more exotic topics where necessary.</p>
</blockquote>

<h2 id="introduction">Introduction</h2>

<p>The goal in supervised learning is to make predictions from data. We start with an initial dataset for which we know what the outcome should be, and our algorithms try and recognize patterns in the data which are unique for each outcome. For example, one popular application of supervised learning is email spam filtering. Here, an email (the data instance) needs to be classified as spam or not-spam.</p>

<p>Following the approach of traditional computer science, one might be tempted to write a carefully designed program that follows some rules to decide if an email is spam or not. Although such a program might work reasonably well for a while, it has significant drawbacks. As email spam changes the program would have to be rewritten. Spammers could attempt to reverse engineer the software and design messages that circumvent it. And even if it is successful, it could probably not easily be applied to different languages.</p>

<figure class="image" style="text-align: center;">
  <img style="max-width: 75%" src="/assets/images/ml_course/spam_diagram.png" alt="" />
  <figcaption></figcaption>
</figure>

<p>Machine Learning uses a different approach to generate a program that can make predictions from data. Instead of programming it by hand it is learned from past data. This process works if we have data instances for which we know exactly what the right prediction would have been. For example past data might be user-annotated as spam or not-spam. A machine learning algorithm can utilize such data to learn a program, a classifier, to predict the correct label of each annotated data instance.</p>

<p>Other successful applications of machine learning include web-search ranking (predict which web-page the user will click on based on his/her search query), placing of online advertisements (predict the expected revenue of an ad, when placed on a homepage, which is seen by a specific user), visual object recognition (predict which object is in an image - e.g. a camera mounted on a self-driving car), face-detection (predict if an image patch contains a human face or not).</p>

<h2 id="the-basics">The Basics</h2>

<p>All supervised learning algorithms start with some dataset \(D = \{(\textbf{x}_1,y_1),\dots,(\textbf{x}_n,y_n)\}\), where each \(x_i\) is a d-dimensional input or feature vector, and \(y_i\) the corresponding output we call our label. We assume that these data points are drawn from some unknown distribution \(\mathcal{P}\), so</p>

\[(\textbf{x}_i,y_i) \sim \mathcal{P}\]

<p>where we want want our \((\textbf{x}_i,y_i)\) to be independent and identically distributed (called iid).</p>

<p>This can be formalized by saying:</p>

\[D = \{(\textbf{x}_1,y_1),\dots,(\textbf{x}_n,y_n)\} \subseteq \mathbb{R}^d \times \mathcal{C}\]

<p>where</p>

<ul>
  <li>\(n\) is the size of our dataset</li>
  <li>\(\mathbb{R}^d\) is the d-dimensional feature space</li>
  <li>\(\textbf{x}_i\) is the feature vector of the \(i^{th}\) example</li>
  <li>\(y_i\) is the label or output of the \(i^{th}\) example</li>
  <li>\(\mathcal{C}\) is the space of all possible labels, or label space.</li>
</ul>

<p>We can sum up the goal of supervised machine learning as finding a function \(h:\mathbb{R}^d \rightarrow \mathcal{C}\), such that for every new input/output pair \((\textbf{x},y)\) sampled from \(\mathcal{P}\) we have \(h(\textbf{x}) \approx y\).</p>

<p>Let’s see if we understand the definitions above by first looking at a few examples of feature spaces and label spaces.</p>

<h3 id="label-spaces-examples">Label Spaces Examples</h3>

<ul>
  <li><em>Binary Classification</em>: Say we’re building a spam filter. Here we have to classes, spam and not spam. The label space is often \(\{0,1\}\) or \(\{-1,1\}\). The choice impacts how we write our loss function, but we’ll see more on that later on.</li>
  <li><em>Multi-class Classification</em>: If we want to build an image classifier, we need to specify which classes we’re interested in (e.g. 1=<em>horse</em>, 2=<em>dog</em>, etc.). If we have \(K\) image classes, we have \(\mathcal{C}=\{1,2,\dots,K\}\).</li>
  <li><em>Regression</em>: If we want to predict the daily temperature, we’re predicting a number which could take any value, even if some are highly improbable. In this case \(\mathcal{C} = \mathbb{R}\).</li>
</ul>

<h3 id="feature-spaces-examples">Feature Spaces Examples</h3>

<ul>
  <li><em>House</em>: If we’re building a model to predict house sale prices, we might take \(\textbf{x}_i = (x_i^1,x_i^2,\dots,x_i^d)\) where \(x_i^1\) is the surface area in \(m^2\), \(x_i^2\) is the number of years ago the house was built, longitude and latitude, etc. In this case we have “hand-crafted” features, each chosen by the modeler.</li>
  <li><em>Text Document</em>: For something like email classification, a common feature space is the so called bag-of-words. First we find all \(d\) unique words over all the documents we have. We then create the vector \(\textbf{x}_i = (x_i^1,x_i^2,\dots,x_i^d)\) for each document \(i\), where each element \(x_i^j\) tells us how often word \(j\) appears in document \(i\).</li>
</ul>

<h2 id="hypothesis-classes-and-no-free-lunch">Hypothesis Classes and No Free Lunch</h2>

<p>There are some steps we need to take on our path to finding that mysterious function $h$. A very important one is that we need to make some assumptions on what the $h$ looks like, and what space of functions we’ll be looking in. This could be linear functions, decision trees, polynomials, or whatever. These are called <em>hypothesis spaces</em>, usually denoted with $\mathcal{H}$. We need to make this assumption, since this choice has a big impact on how our model will generalize to new data points which aren’t present in our training data, which is the ultimate goal of machine learning.</p>

<p>One way of understanding these hypothesis spaces is to look at how expressive they are, or how flexible they are in capturing the structure of a dataset. One of the challenges of machine learning is finding the right balance of flexibility, where not enough causes <em>underfitting</em> or <em>bias</em>, and too much causes <em>overfitting</em> or <em>variance</em>. One way of describing this expressiveness is the <a href="https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension">Vapnik–Chervonenkis dimension</a>.</p>

<figure class="image" style="text-align: center;">
  <img style="" src="/assets/images/ml_course/hypothesis_space_impact.png" alt="Here we see the impact of choosing our hypothesis space, also known as the bias-variance tradeoff. If our space is too large (picture on right) we get fantastic results on our training set, but horrible generalization. If our space is too small (picture on left), we get bad results on the training set, and bad generalization." />
  <figcaption>Here we see the impact of choosing our hypothesis space, also known as the bias-variance tradeoff. If our space is too large (picture on right) we get fantastic results on our training set, but horrible generalization. If our space is too small (picture on left), we get bad results on the training set, and bad generalization.</figcaption>
</figure>

<p>Each machine learning algorithm makes assumptions which restrict its search to a specific space of functions. There’s no way around this due to the <a href="https://www.wikiwand.com/en/No_free_lunch_in_search_and_optimization">No Free Lunch Theorem</a>, which you could summarize as “there’s no ultimate ML algorithm which is the best on all problems”.</p>

<h2 id="loss-functions">Loss Functions</h2>

<p>After having been given a dataset \(D\) and having chosen a hypothesis space \(\mathcal{H}\) of functions we can possibly learn, the next step is to find the best \(h\) in that set. The problem is, how do we define best \(h\)?</p>

<p>This is where a loss function \(L:\mathcal{H} \rightarrow \mathbb{R}\) comes into play, which assigns a loss to each \(h \in \mathcal{H}\). This loss number tells us how good our \(h\) is, given the data \(D\) we want it to reproduce. In general a loss function will give hypotheses \(h\) with fewer reproduction errors on \(D\) a lower loss, but there are times where you want to take additional factors into account like model complexity. We’ll return to this when we cover regularization.</p>

<p>With this loss function at hand, our original problem has now become an optimization problem, which is</p>

\[\arg\min_{h \in \mathcal{H}} L(h) = \arg\min_{h \in \mathcal{H}} \frac{1}{n}\sum_{i=1}^nl(\textbf{x}_i,y|h)\]

<p>We’ll be using \(L\) for the loss of a hypothesis \(h\) given \(D\), and \(l\) for the loss of a single data pair \((\textbf{x}_i,y)\) given \(h\).</p>

<h3 id="zero-one-loss">Zero-One Loss</h3>

<p>This is one of the simplest loss functions. What this loss does is count the number of mistakes \(h\) makes for each training sample. We can state this as:</p>

\[L(h) := \frac{1}{n}\sum_{i=1}^n\delta_{h(\textbf{x}_i)\neq y_i}\]

<p>where \(\delta\) is the Dirac delta function</p>

\[\delta_{h(\textbf{x}_i)\neq y_i} = \begin{cases} 
      1 &amp; h(\textbf{x}_i)\neq y_i \\
      0 &amp; \text{otherwise} \\
   \end{cases}\]

<p>This isn’t used much in practice since it’s non-differentiable and non-continuous, which makes it difficult to work with for optimization algorithms we be needing.</p>

<h3 id="squared-loss">Squared Loss</h3>

<p>This loss is generally used in regression problems where \(y_i \in \mathbb{R}\). The loss per training sample is \((h(\mathbf{x}_i) - y_i)^2\), which is the distance squared. The overall loss becomes</p>

\[L(w):=\frac{1}{n}\sum_{i=1}^n(h(\mathbf{x}_i) - y_i)^2\]

<p>The fact that the error is squared means that large errors will be much more punishing than smaller ones, and when searching for \(h \in \mathcal{H}\) we’ll end up choosing one which would rather have lots of small errors rather than a few large ones.</p>

<h2 id="generalization">Generalization</h2>

<p>One question you might want to ask is “if we’re only looking at our data \(D\), how do we ensure our model will perform well on new data?” We’ll show why this is an important question with a little motivating example.</p>

<p>Given a data set \(D\), lets define a function \(h\) which just memorizes the output \(y_i\) for each input vector \(\textbf{x}_i\), or:</p>

\[h(\textbf{x}) := \begin{cases} y_i &amp; \text{if there exists $(\textbf{x}_i,y_i)$ such that $\textbf{x} = \textbf{x}_i$}, \\ 0 &amp; \text{otherwise}\end{cases}\]

<p>This function would perform perfectly on our training data \(D\), but anything new which we haven’t seen before would probably be horribly wrong. So how do we ensure we don’t learn hypotheses like this?</p>

<h3 id="train-test-split">Train Test Split</h3>

<p>The generally used approach to avoid the above pitfall is to split our dataset \(D\) into three sets, \(D_{TR}, D_{VA}, D_{TE}\), which are usually called train, validation and test. A good split might be something like 80/10/10, although this depends on the application and size of \(D\) among other things.</p>

<figure class="image" style="text-align: center;">
  <img style="" src="/assets/images/ml_course/train_validation_test.png" alt="" />
  <figcaption></figcaption>
</figure>

<p>We do this at the very start of our search for \(h\). When we’re trying out all kinds of different algorithms we use the training set to search for our function \(h\), and our validation set to determine if it’s any good. Once we’ve done this numerous times and are happy with the results, we crack open the test set to see what the final accuracy is.</p>

<blockquote>
  <p><strong>Train/Test Split Gone Wrong</strong></p>

  <p>We need to be really careful when we’re creating the split in our data. Lets take spam-filtering as an example. If we just randomly split our emails and labels into train/validation/test, create a classifier, we’d we surprised to find that our model has 0% error! Although some might think “Great, we’re done!”, it would be wiser to look into what might be happening.</p>

  <p>After some detective work, you would discover we made a big error. Spammers create an email once and send it to millions of people. This means we have same email text in both our training set and test set. What our classifier did was just memorize which words are in those spam emails, and since the same text was in the test set it worked great.</p>

  <p>In reality spammers change their emails frequently to avoid such classifiers, so we should be smarter too. We want our test set to mimick the settings in which our algorithm will have to function in the real world. A simple way to fix this is to split our dataset by time, so we’ll be testing on newer emails than were found in our training set.</p>
</blockquote>

<h3 id="formalizing-generalization">Formalizing Generalization</h3>

<p>We can formalize what we mean by generalization with this expression</p>

\[\mathbb{E}_{(\textbf{x},y)\sim \mathcal{P}}\left[l(h;(\textbf{x},y))\right]\]

<p>which is the expected loss of our hypothesis, if we take the expectation over all possible input/output pairs drawn from \(\mathcal{P}\). This is really what we want to minimize, but we can’t do this directly since we don’t know what \(\mathcal{P}\) is.</p>

<p>This is what we’re trying to simulate with our validation set \(D_{VA}\). Since our algorithm has never seen this set, it’s as though we’re drawing that many new points from our distribution \(\mathcal{P}\)!</p>

<h2 id="summary">Summary</h2>

<p>We defined our dataset $D$ as a set \(\{(\textbf{x}_1,y_1),\dots,(\textbf{x}_n,y_n)\}\) of data points. Given this dataset, we need to choose a hypothesis space \(\mathcal{H}\) which we will search through to find a good function \(h \in \mathcal{H}\) for which \(h(\textbf{x}) \approx y\) when \((\textbf{x},y) \sim \mathcal{P}\). We generally choose \(\mathcal{H}\) implicitly by our choice of machine learning algorithm.</p>

<p>Which function \(h\) is best, is decided by our choice of loss function $L$. This allows us to compare the performance of datasets on some set of data points.</p>

<p>In order to make sure our algorithm generalizes well after training, we split our dataset into trainging, validation and test. We training on the training set, optimize our algorithm according to the loss on the validation set, and once we’re done we use the test set to get a good approximation of how our algorithm will perform in the real world.</p>

  </div>

  <hr style="margin-top: 3rem;">

  <h3 data-toc-skip>Sharing is caring.</h3>
  <div class="a2a_kit a2a_kit_size_30 a2a_default_style" style="margin: 0rem 0rem;">
  <a class="a2a_button_twitter a2a_counter"></a>
  <a class="a2a_button_linkedin a2a_counter"></a>
  <a class="a2a_button_hacker_news a2a_counter"></a>
  <a class="a2a_button_whatsapp a2a_counter"></a>
  <a class="a2a_button_reddit a2a_counter"></a>
  <a class="a2a_button_facebook a2a_counter"></a>
  </div>

  <div class="wrapper">
  <h3 data-toc-skip> Did you like the article? Subscribe for more.</h3>
      <form action="https://dev.us20.list-manage.com/subscribe/post-json?u=fb1deb852e0da99410d8eb6b2&amp;id=8acc3cf1bd&c=?"
          method="get" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate">
          <input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="Email Address here" required>
          <div style="position: absolute; left: -5000px;" aria-hidden="true">
              <input type="text" name="b_e44c1f194bec93e238615469e_f6f826e769" tabindex="-1" value="">
          </div>
          <input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="btn btn-secondary">
          <div id="subscribe-result">
          </div>
      </form>
</div>

<script>
  $(document).ready(function () {
    var $form = $('#mc-embedded-subscribe-form')
    if ($form.length > 0) {
      $('form input[type="submit"]').bind('click', function (event) {
        if (event) event.preventDefault()
        register($form)
      })
    }
  })

  function register($form) {
    $('#mc-embedded-subscribe').val('Sending...');
    $.ajax({
      type: $form.attr('method'),
      url: $form.attr('action'),
      data: $form.serialize(),
      cache: false,
      dataType: 'json',
      contentType: 'application/json; charset=utf-8',
      error: function (err) { alert('Could not connect to the registration server. Please try again later.') },
      success: function (data) {
        $('#mc-embedded-subscribe').val('Subscribe')
        if (data.result === 'success') {
          // Yeahhhh Success
          console.log(data.msg)
          $('#mc-embedded-subscribe').removeClass("btn-secondary").addClass("btn-success");
          $('#mc-embedded-subscribe').val('Success!')
          $('#mce-EMAIL').css('borderColor', '#ffffff')
          $('#subscribe-result').css('color', 'rgb(53, 114, 210)')
          $('#subscribe-result').html('<p>Thank you for subscribing.</p>')
          $('#mce-EMAIL').val('')
        } else {
          // Something went wrong, do something to notify the user.
          console.log(data.msg)
          $('#mce-EMAIL').css('borderColor', '#ff8282')
          $('#subscribe-result').css('color', '#ff8282')
          $('#subscribe-result').html('<p>' + data.msg + '</p>')
        }
      }
    })
  };
</script>


  <script async src="https://static.addtoany.com/menu/page.js"></script>
  <script>
    var a2a_config = a2a_config || {};
    a2a_config.counts = { recover_domain: 'dorianbrown.github.io' };
  </script>

</article>

  </div>
</div>

  </div>

  <div class="wrapper-footer">
    <div class="container">
      <footer class="footer">
        <a href="mailto:dorianstuartbrown@gmail.com"><i class="svg-icon email"></i></a>
<a href="https://github.com/dorianbrown"><i class="svg-icon github"></i></a>
<a href="https://www.linkedin.com/in/doriansbrown"><i class="svg-icon linkedin"></i></a>
<a href="http://stackoverflow.com/users/1415371/ballzoffury?tab=profile"><i class="svg-icon stackoverflow"></i></a>

      </footer>
    </div>
  </div>
</div>

<!-- Lightgallery call -->
<script type="text/javascript">
  $(document).ready(function () {
    $(function(){

      $('a').hover(function(e){

          $(this).attr('data-title', $(this).attr('title'));
          $(this).removeAttr('title');

      },
      function(e){

          $(this).attr('title', $(this).attr('data-title'));

      });
    });
    //By default, plugin uses `data-fancybox-group` attribute to create galleries.
    $(".fancybox").jqPhotoSwipe({
      galleryOpen: function (gallery) {
        //with `gallery` object you can access all methods and properties described here http://photoswipe.com/documentation/api.html
        //console.log(gallery);
        //console.log(gallery.currItem);
        //console.log(gallery.getCurrentIndex());
        //gallery.zoomTo(1, {x:gallery.viewportSize.x/2,y:gallery.viewportSize.y/2}, 500);
        gallery.toggleDesktopZoom();
      }
    });
    //This option forces plugin to create a single gallery and ignores `data-fancybox-group` attribute.
    $(".forcedgallery > a").jqPhotoSwipe({
      forceSingleGallery: true
    });
  });
</script>
  
<script type="text/javascript">
  $(document).ready(function(){
    $(".wrapper-masthead").each(function(){
      var headroom = new Headroom(this);
      headroom.init();
    })
  })
</script>

<script type="text/javascript">
  $(document).ready(function () {
  });
</script>

<script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script>
<script type="text/javascript">
  function showSubscribeModal() {
    window.dojoRequire(["mojo/signup-forms/Loader"], 
      function(L) { 
        L.start({
          "baseUrl":"mc.us20.list-manage.com",
          "uuid":"fb1deb852e0da99410d8eb6b2",
          "lid":"8acc3cf1bd",
          "uniqueMethods":true
      }) 
      document.cookie = 'MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC;';
      document.cookie = 'MCPopupSubscribed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC;';
    })
  }

  $('#subscribe-button').click(showSubscribeModal)
</script>



	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-107934583-2', 'auto');
		ga('send', 'pageview', {
		  'page': '/what-is-supervised-learning/',
		  'title': 'What is Supervised Learning? A Mathematical Perspective'
		});
	</script>
	<!-- End Google Analytics -->



</body>
</html>
